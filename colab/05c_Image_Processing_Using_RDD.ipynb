{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suriarasai/BEAD2025/blob/main/colab/05c_Image_Processing_Using_RDD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRUfHuH-KZ-u"
      },
      "source": [
        "This notebook demonstrates the core RDD-based approach, which is excellent for custom, low-level data transformations on unstructured data like images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ry4nyaLOKd6B"
      },
      "source": [
        "### PySpark Setp\n",
        "\n",
        "The first step involves installing pyspark.  The next step is to install findspark library.\n",
        "\n",
        "*Note: the --ignore-install flag is used to ignore previous installations and use the latest one built alongside the allocated cluster.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjHMeUn7JiYj",
        "outputId": "d874df5c-69ee-4ac8-ab90-7b661f0ab345"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Selecting previously unselected package openjdk-21-jre-headless:amd64.\n",
            "(Reading database ... 126371 files and directories currently installed.)\n",
            "Preparing to unpack .../openjdk-21-jre-headless_21.0.8+9~us1-0ubuntu1~22.04.1_amd64.deb ...\n",
            "Unpacking openjdk-21-jre-headless:amd64 (21.0.8+9~us1-0ubuntu1~22.04.1) ...\n",
            "Selecting previously unselected package openjdk-21-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-21-jdk-headless_21.0.8+9~us1-0ubuntu1~22.04.1_amd64.deb ...\n",
            "Unpacking openjdk-21-jdk-headless:amd64 (21.0.8+9~us1-0ubuntu1~22.04.1) ...\n",
            "Setting up openjdk-21-jre-headless:amd64 (21.0.8+9~us1-0ubuntu1~22.04.1) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/java to provide /usr/bin/java (java) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jpackage to provide /usr/bin/jpackage (jpackage) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/keytool to provide /usr/bin/keytool (keytool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/rmiregistry to provide /usr/bin/rmiregistry (rmiregistry) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/lib/jexec to provide /usr/bin/jexec (jexec) in auto mode\n",
            "Setting up openjdk-21-jdk-headless:amd64 (21.0.8+9~us1-0ubuntu1~22.04.1) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jar to provide /usr/bin/jar (jar) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jarsigner to provide /usr/bin/jarsigner (jarsigner) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/javac to provide /usr/bin/javac (javac) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/javadoc to provide /usr/bin/javadoc (javadoc) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/javap to provide /usr/bin/javap (javap) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jcmd to provide /usr/bin/jcmd (jcmd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jdb to provide /usr/bin/jdb (jdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jdeprscan to provide /usr/bin/jdeprscan (jdeprscan) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jdeps to provide /usr/bin/jdeps (jdeps) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jfr to provide /usr/bin/jfr (jfr) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jimage to provide /usr/bin/jimage (jimage) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jinfo to provide /usr/bin/jinfo (jinfo) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jlink to provide /usr/bin/jlink (jlink) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jmap to provide /usr/bin/jmap (jmap) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jmod to provide /usr/bin/jmod (jmod) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jps to provide /usr/bin/jps (jps) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jrunscript to provide /usr/bin/jrunscript (jrunscript) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jshell to provide /usr/bin/jshell (jshell) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jstack to provide /usr/bin/jstack (jstack) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jstat to provide /usr/bin/jstat (jstat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jstatd to provide /usr/bin/jstatd (jstatd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jwebserver to provide /usr/bin/jwebserver (jwebserver) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/serialver to provide /usr/bin/serialver (serialver) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jhsdb to provide /usr/bin/jhsdb (jhsdb) in auto mode\n",
            "java-1.21.0-openjdk-amd64\n",
            "java-21-openjdk-amd64\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# 1. Install OpenJDK 21 (if not already done in a previous cell)\n",
        "!apt-get update -qq\n",
        "!apt-get install -qq openjdk-21-jdk-headless\n",
        "\n",
        "# 2. Verify where it landed (if needed)\n",
        "!ls /usr/lib/jvm | grep 21\n",
        "\n",
        "# 3. Point to JDK 21\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-21-openjdk-amd64\"\n",
        "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
        "\n",
        "# 4. Install PySpark via pip (make sure this happens AFTER setting JAVA_HOME)\n",
        "!pip install pyspark --quiet\n",
        "\n",
        "# 5. Import and start Spark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = (\n",
        "    SparkSession.builder\n",
        "      .master(\"local[*]\")\n",
        "      .appName(\"Spark on Java21\")\n",
        "      .getOrCreate()\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7qJXvn8K2o0"
      },
      "source": [
        "Next, we need to set up the Kaggle API to download our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Mp_m4igdLCO0"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install kaggle -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QIM4sS_LIxy"
      },
      "source": [
        "### Configure Kaggle API\n",
        "To use the Kaggle API, you need your API token.\n",
        "\n",
        "Go to Kaggle account page: https://www.kaggle.com/your-username/account\n",
        "\n",
        "Click on \"Create New API Token\". This will download a kaggle.json file.\n",
        "\n",
        "Run the next cell and upload that kaggle.json file when prompted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "KeNrbnEkLxHf",
        "outputId": "9bf4153f-f27a-40e6-910d-5f0b9426a882"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload your kaggle.json file\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7740664c-e945-461e-9a61-2bc15c6e67d1\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7740664c-e945-461e-9a61-2bc15c6e67d1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        }
      ],
      "source": [
        "# Upload your kaggle.json file\n",
        "from google.colab import files\n",
        "print('Please upload your kaggle.json file')\n",
        "files.upload()\n",
        "\n",
        "# Move the file to the required directory and set permissions\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ey5uFHECL1c4"
      },
      "source": [
        "### Download and Unzip the Dataset\n",
        "We'll use the \"Intel Image Classification\" dataset, which is well-structured and a manageable size. It contains images of natural scenes categorized into folders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVv3IU4RMW_Y",
        "outputId": "93938abd-3be1-4cdc-a72a-774cbe69602b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/puneet6060/intel-image-classification\n",
            "License(s): copyright-authors\n",
            "Downloading intel-image-classification.zip to /content\n",
            " 92% 319M/346M [00:00<00:00, 517MB/s]\n",
            "100% 346M/346M [00:00<00:00, 431MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Download the dataset from Kaggle\n",
        "!kaggle datasets download -d puneet6060/intel-image-classification\n",
        "\n",
        "# Unzip the dataset quietly\n",
        "!unzip -q intel-image-classification.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vc0VJoBqMi1m"
      },
      "source": [
        "### Load Images as an RDD\n",
        "The key to reading binary files like images is the sc.binaryFiles() method. It creates an RDD where each element is a tuple (filepath, PortableDataStream). The PortableDataStream contains the raw image bytes."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y Pillow\n",
        "!pip install Pillow"
      ],
      "metadata": {
        "id": "1IXQ_hsFfrM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So first test if the Pillow library works as expected."
      ],
      "metadata": {
        "id": "pAXwzS_7fjx2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Define a path to a single, specific image\n",
        "single_image_path = 'seg_train/seg_train/buildings/0.jpg'\n",
        "\n",
        "print(f\"Attempting to process one image: {single_image_path}\")\n",
        "\n",
        "try:\n",
        "    # Try to open the image from the file system\n",
        "    with open(single_image_path, 'rb') as f:\n",
        "        image = Image.open(f)\n",
        "\n",
        "        # Perform the same transformations\n",
        "        processed_image = image.convert('L').resize((64, 64))\n",
        "        image_array = np.array(processed_image)\n",
        "\n",
        "        print(f\"âœ… Success! Single image processed correctly.\")\n",
        "        print(f\"Image mode: {processed_image.mode}, Size: {processed_image.size}, Shape: {image_array.shape}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"ðŸ›‘ FAILED: The file was not found at '{single_image_path}'. Make sure the dataset was unzipped correctly.\")\n",
        "except Exception as e:\n",
        "    print(f\"ðŸ›‘ FAILED TO PROCESS SINGLE IMAGE. THIS IS THE KEY ERROR:\")\n",
        "    print(f\"   Error Type: {type(e).__name__}\")\n",
        "    print(f\"   Error Message: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmIHwejJfKV5",
        "outputId": "c386d4e4-2bb8-40af-c984-604728ab2ad2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to process one image: seg_train/seg_train/buildings/0.jpg\n",
            "âœ… Success! Single image processed correctly.\n",
            "Image mode: L, Size: (64, 64), Shape: (64, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3T8OV8WGOCjw",
        "outputId": "9cc8f9ac-a1a3-4af8-e80b-9b8803f68fe1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verifying directory contents:\n",
            "10007.jpg\n",
            "10010.jpg\n",
            "10020.jpg\n",
            "10030.jpg\n",
            "10037.jpg\n",
            "\n",
            "--- Processing RDD ---\n",
            "File Path: file:/content/seg_train/seg_train/forest/17180.jpg\n",
            "Data Type: <class 'bytes'>\n",
            "A sample of image RDD has 2271 images.\n"
          ]
        }
      ],
      "source": [
        "# Define the correct path to the training images\n",
        "# The path is relative to your notebook's location, which is /content\n",
        "image_dir = \"seg_train/seg_train/forest\"\n",
        "\n",
        "# Load all images from the directory and its subdirectories into an RDD\n",
        "# The result is an RDD of (file_path, binary_content)\n",
        "image_rdd = spark.sparkContext.binaryFiles(image_dir)\n",
        "\n",
        "# --- Verification ---\n",
        "# You can confirm the path is correct by listing its contents\n",
        "print(\"Verifying directory contents:\")\n",
        "!ls {image_dir} | head -n 5\n",
        "\n",
        "# Let's inspect the first element to see the structure\n",
        "print(\"\\n--- Processing RDD ---\")\n",
        "first_element = image_rdd.take(1)[0]\n",
        "print(f\"File Path: {first_element[0]}\")\n",
        "print(f\"Data Type: {type(first_element[1])}\")\n",
        "print(f\"A sample of image RDD has {image_rdd.count()} images.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ofx_LBkxPNLa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c32a80a-feee-45b8-b843-70b36aa7cb0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Successfully processed 2271 images.\n",
            "\n",
            "--- Sample of Processed RDD ---\n",
            "Label: forest, Image Array Shape: (64, 64), DType: uint8\n",
            "Label: forest, Image Array Shape: (64, 64), DType: uint8\n",
            "\n",
            "--- Image Count Per Category ---\n",
            "- forest: 2271 images\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import io\n",
        "import numpy as np\n",
        "\n",
        "def process_image(element):\n",
        "    \"\"\"\n",
        "    Parses an RDD element to process an image, correctly handling the 'bytes' object.\n",
        "\n",
        "    Args:\n",
        "        element: A tuple containing the file path and the image data as bytes.\n",
        "\n",
        "    Returns:\n",
        "        A tuple of (label, numpy_array) or None if processing fails.\n",
        "    \"\"\"\n",
        "    filepath, data_stream = element\n",
        "\n",
        "    try:\n",
        "        # 1. Extract the label from the parent directory's name\n",
        "        label = os.path.basename(os.path.dirname(filepath))\n",
        "\n",
        "        # 2. Read the image bytes into a Pillow Image object\n",
        "        # THE FIX IS HERE: We remove .readAll() because data_stream is already 'bytes'\n",
        "        image = Image.open(io.BytesIO(data_stream))\n",
        "\n",
        "        # 3. Perform transformations\n",
        "        processed_image = image.convert('L').resize((64, 64))\n",
        "\n",
        "        # 4. Convert the processed image to a NumPy array\n",
        "        image_array = np.array(processed_image)\n",
        "\n",
        "        return (label, image_array)\n",
        "\n",
        "    except Exception as e:\n",
        "        # This should no longer happen, but it's good practice to keep it\n",
        "        print(f\"Could not process {filepath}: {e}\")\n",
        "        return None\n",
        "\n",
        "# --- Apply the Corrected Function and Analyze Results ---\n",
        "\n",
        "# Use map() to apply our processing function to each element of the RDD\n",
        "processed_rdd = image_rdd.map(process_image)\n",
        "\n",
        "# Use filter() to remove any images that failed processing\n",
        "processed_rdd = processed_rdd.filter(lambda x: x is not None)\n",
        "\n",
        "# Cache the RDD in memory for faster access\n",
        "processed_rdd.cache()\n",
        "\n",
        "# Action 1: Count the total number of successfully processed images\n",
        "total_images = processed_rdd.count()\n",
        "print(f\"âœ… Successfully processed {total_images} images.\")\n",
        "\n",
        "# Action 2: Inspect the first 2 elements of our processed RDD\n",
        "print(\"\\n--- Sample of Processed RDD ---\")\n",
        "sample_data = processed_rdd.take(2)\n",
        "for label, img_array in sample_data:\n",
        "    print(f\"Label: {label}, Image Array Shape: {img_array.shape}, DType: {img_array.dtype}\")\n",
        "\n",
        "\n",
        "# Action 3: Get a count of images per category\n",
        "label_counts = processed_rdd.map(lambda x: x[0]).countByValue()\n",
        "\n",
        "print(\"\\n--- Image Count Per Category ---\")\n",
        "for label, count in sorted(label_counts.items()):\n",
        "    print(f\"- {label}: {count} images\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Conclusion and Cleanup\n",
        "\n",
        "This example demonstrates the fundamental RDD workflow for custom data processing. We loaded raw binary files, applied a complex Python function using map(), and performed aggregations.\n",
        "\n",
        "For more structured machine learning tasks, the next step would often be to convert this RDD into a Spark DataFrame, which integrates seamlessly with Spark's MLlib library. However, for custom ETL (Extract, Transform, Load) and preprocessing, the RDD API provides maximum flexibility.\n",
        "\n",
        "Finally, it's good practice to stop the Spark session to release resources."
      ],
      "metadata": {
        "id": "pnq3_gG3gBYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stop the Spark session\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "y3z-rTijgAz4"
      },
      "execution_count": 17,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOmonE0A3Hgs/HqCHKaIuUh",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}