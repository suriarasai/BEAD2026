{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO/DqEGlmq6MMKyzcykyyMz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suriarasai/BEAD2026/blob/main/colab/03b_DataMungingUsingFunctions_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Munging Examples\n",
        "Data munging (or data wrangling) in PySpark for taxi booking services typically involves cleaning, transforming, and enriching raw data to make it suitable for analysis.\n",
        "Here are some common data munging tasks with examples:"
      ],
      "metadata": {
        "id": "Qi8hBz_n9l3M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Spark\n",
        "(ONLY IF SPARK DOESNT WORK)"
      ],
      "metadata": {
        "id": "J7FtUeDXXrY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install pyspark using pip\n",
        "!pip install --ignore-install -q pyspark\n",
        "# install findspark using pip\n",
        "!pip install --ignore-install -q findspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-KDz-CkXtey",
        "outputId": "041f9646-fccf-4114-aa43-b41571f6241c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SparkSession\n",
        "Start Spark Session"
      ],
      "metadata": {
        "id": "6qYGMkfBXt9y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zUYiRid59VYB"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "# import collections\n",
        "spark = SparkSession.builder.master(\"local\").appName(\"My First PySpark App\").getOrCreate()\n",
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Drive\n",
        "Connect to Google Drive"
      ],
      "metadata": {
        "id": "9PWsaqGeYJ6y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to read in data from a text file, first upload the data file into your google drive and then mount your google drive onto colab\n",
        "from google.colab import drive\n",
        "# to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True)\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEzzDmlBcrNH",
        "outputId": "7994668e-9e1e-45c9-de82-d2bb15e2ae9d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Taxi Data Munging\n",
        "\n",
        "### Munging with RDD\n",
        "RDDs are lower-level and require more manual handling, but they give you more control over the data processing.\n",
        "\n",
        "Data munging using PySpark RDDs requires manual transformation functions. We'll work with tuples representing (trip_id, pickup_time, dropoff_time, dropoff_location, fare_amount)\n",
        "\n",
        "### Creating Sample Data"
      ],
      "metadata": {
        "id": "DiyLarv1k8TY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample Data as tuples\n",
        "data = [\n",
        "    (1, \"2025-02-18 10:00:00\", \"2025-02-18 10:30:00\", None, 15.5),\n",
        "    (2, \"2025-02-18 11:00:00\", None, \"Downtown\", 20.0),\n",
        "    (3, None, \"2025-02-18 12:00:00\", \"Airport\", None),\n",
        "]\n",
        "\n",
        "# Create RDD\n",
        "rdd = sc.parallelize(data)\n",
        "\n",
        "# Display the RDD\n",
        "print(\"Original RDD:\")\n",
        "for row in rdd.collect():\n",
        "    print(row)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4PNzLqzloQn",
        "outputId": "1f6a6cf3-a5c2-4857-9122-96897bd80e1a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original RDD:\n",
            "(1, '2025-02-18 10:00:00', '2025-02-18 10:30:00', None, 15.5)\n",
            "(2, '2025-02-18 11:00:00', None, 'Downtown', 20.0)\n",
            "(3, None, '2025-02-18 12:00:00', 'Airport', None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handling Missing Values\n",
        "Replace None values with default values using map transformation"
      ],
      "metadata": {
        "id": "oibkN5PDmBqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fill_missing_values(row):\n",
        "    \"\"\"\n",
        "    Fill missing values in a row\n",
        "    Row format: (trip_id, pickup_time, dropoff_time, dropoff_location, fare_amount)\n",
        "    \"\"\"\n",
        "    trip_id, pickup_time, dropoff_time, dropoff_location, fare_amount = row\n",
        "\n",
        "    # Replace None with default values\n",
        "    dropoff_location = dropoff_location if dropoff_location is not None else \"Unknown\"\n",
        "    fare_amount = fare_amount if fare_amount is not None else 0.0\n",
        "\n",
        "    return (trip_id, pickup_time, dropoff_time, dropoff_location, fare_amount)\n",
        "\n",
        "# Apply transformation\n",
        "rdd_cleaned = rdd.map(fill_missing_values)\n",
        "\n",
        "print(\"\\nRDD after filling missing values:\")\n",
        "for row in rdd_cleaned.collect():\n",
        "    print(row)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSv-76kKmC6L",
        "outputId": "3a992e68-1047-424c-ea8d-e784e98cb678"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "RDD after filling missing values:\n",
            "(1, '2025-02-18 10:00:00', '2025-02-18 10:30:00', 'Unknown', 15.5)\n",
            "(2, '2025-02-18 11:00:00', None, 'Downtown', 20.0)\n",
            "(3, None, '2025-02-18 12:00:00', 'Airport', 0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Converting Datetime Formats\n",
        "Parse string timestamps to datetime objects\n"
      ],
      "metadata": {
        "id": "78viZmkKmLo_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_timestamps(row):\n",
        "    \"\"\"\n",
        "    Convert timestamp strings to datetime objects\n",
        "    \"\"\"\n",
        "    trip_id, pickup_time, dropoff_time, dropoff_location, fare_amount = row\n",
        "\n",
        "    # Parse timestamps\n",
        "    if pickup_time:\n",
        "        try:\n",
        "            pickup_time = datetime.strptime(pickup_time, \"%Y-%m-%d %H:%M:%S\")\n",
        "        except:\n",
        "            pickup_time = None\n",
        "\n",
        "    if dropoff_time:\n",
        "        try:\n",
        "            dropoff_time = datetime.strptime(dropoff_time, \"%Y-%m-%d %H:%M:%S\")\n",
        "        except:\n",
        "            dropoff_time = None\n",
        "\n",
        "    return (trip_id, pickup_time, dropoff_time, dropoff_location, fare_amount)\n",
        "\n",
        "# Apply transformation\n",
        "rdd_with_timestamps = rdd.map(parse_timestamps)\n",
        "\n",
        "print(\"\\nRDD with parsed timestamps:\")\n",
        "for row in rdd_with_timestamps.collect():\n",
        "    print(row)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k47oJ4c7mQwm",
        "outputId": "9d50568a-44da-43ce-b9b4-d7d5fae344be"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "RDD with parsed timestamps:\n",
            "(1, None, None, None, 15.5)\n",
            "(2, None, None, 'Downtown', 20.0)\n",
            "(3, None, None, 'Airport', None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Filtering Invalid Data\n",
        "Keep only valid trips with positive fares and non-null pickup times"
      ],
      "metadata": {
        "id": "Y1ZaborpmZTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_valid_trip(row):\n",
        "    \"\"\"\n",
        "    Check if a trip is valid\n",
        "    \"\"\"\n",
        "    trip_id, pickup_time, dropoff_time, dropoff_location, fare_amount = row\n",
        "\n",
        "    # Valid if fare > 0 and pickup_time is not None\n",
        "    return fare_amount is not None and fare_amount > 0 and pickup_time is not None\n",
        "\n",
        "# Filter valid trips\n",
        "rdd_valid = rdd_with_timestamps.filter(is_valid_trip)\n",
        "\n",
        "print(\"\\nValid trips:\")\n",
        "for row in rdd_valid.collect():\n",
        "    print(row)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpzwN4rKmbig",
        "outputId": "bccc5d71-f60b-47cc-efe1-78eea7729b2b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Valid trips:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Engineering - Trip Duration\n",
        "Calculate trip duration in minutes"
      ],
      "metadata": {
        "id": "SVgEgELpmi3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_trip_duration(row):\n",
        "    \"\"\"\n",
        "    Calculate trip duration in minutes\n",
        "    Returns: (trip_id, pickup_time, dropoff_time, dropoff_location, fare_amount, trip_duration_mins)\n",
        "    \"\"\"\n",
        "    trip_id, pickup_time, dropoff_time, dropoff_location, fare_amount = row\n",
        "\n",
        "    # Calculate duration if both times are available\n",
        "    if pickup_time and dropoff_time:\n",
        "        duration = (dropoff_time - pickup_time).total_seconds() / 60\n",
        "    else:\n",
        "        duration = None\n",
        "\n",
        "    return (trip_id, pickup_time, dropoff_time, dropoff_location, fare_amount, duration)\n",
        "\n",
        "# Add trip duration\n",
        "rdd_with_duration = rdd_with_timestamps.map(calculate_trip_duration)\n",
        "\n",
        "print(\"\\nRDD with trip duration:\")\n",
        "for row in rdd_with_duration.collect():\n",
        "    print(row)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2gwHgfdmg02",
        "outputId": "32249283-c0eb-4c0d-bfb1-5d483b3a421b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "RDD with trip duration:\n",
            "(1, None, None, None, 15.5, None)\n",
            "(2, None, None, 'Downtown', 20.0, None)\n",
            "(3, None, None, 'Airport', None, None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Geospatial Filtering\n",
        "Filter trips by dropoff location"
      ],
      "metadata": {
        "id": "4A9T0EXtmpP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_location(row, location):\n",
        "    \"\"\"\n",
        "    Check if trip ended at specified location\n",
        "    \"\"\"\n",
        "    trip_id, pickup_time, dropoff_time, dropoff_location, fare_amount, duration = row\n",
        "    return dropoff_location == location\n",
        "\n",
        "# Filter trips to Changi\n",
        "rdd_changi = rdd_with_duration.filter(lambda row: is_location(row, \"Changi\"))\n",
        "\n",
        "print(\"\\nTrips to Changi:\")\n",
        "changi_trips = rdd_changi.collect()\n",
        "if changi_trips:\n",
        "    for row in changi_trips:\n",
        "        print(row)\n",
        "else:\n",
        "    print(\"No trips found to Changi\")\n",
        "\n",
        "# Filter trips to Airport\n",
        "rdd_airport = rdd_with_duration.filter(lambda row: is_location(row, \"Airport\"))\n",
        "print(\"\\nTrips to Airport:\")\n",
        "for row in rdd_airport.collect():\n",
        "    print(row)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8yu6bpAmsqG",
        "outputId": "78e79f92-6e5d-49ae-dc4c-711257163536"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trips to Changi:\n",
            "No trips found to Changi\n",
            "\n",
            "Trips to Airport:\n",
            "(3, None, None, 'Airport', None, None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Aggregation - Daily Revenue\n",
        "Calculate total fare per day using reduceByKey"
      ],
      "metadata": {
        "id": "T33XSC00nM8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_date_and_fare(row):\n",
        "    \"\"\"\n",
        "    Extract date and fare amount for aggregation\n",
        "    Returns: (date_string, fare_amount)\n",
        "    \"\"\"\n",
        "    trip_id, pickup_time, dropoff_time, dropoff_location, fare_amount, duration = row\n",
        "\n",
        "    if pickup_time and fare_amount:\n",
        "        date_str = pickup_time.strftime(\"%Y-%m-%d\")\n",
        "        return (date_str, fare_amount)\n",
        "    return (None, 0.0)\n",
        "\n",
        "# Create pair RDD and aggregate\n",
        "rdd_daily_fare = rdd_with_duration \\\n",
        "    .map(extract_date_and_fare) \\\n",
        "    .filter(lambda x: x[0] is not None) \\\n",
        "    .reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "print(\"\\nDaily total fare:\")\n",
        "for date, total_fare in rdd_daily_fare.collect():\n",
        "    print(f\"Date: {date}, Total Fare: ${total_fare:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9skIUlnnQvx",
        "outputId": "44066d5b-cbfc-4a58-fd13-9af093fde8a0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Daily total fare:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Deduplication\n",
        "Remove duplicate trips based on trip_id"
      ],
      "metadata": {
        "id": "QuTHcypmnTD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Method 1: Using distinct() on the entire row (works if rows are identical)\n",
        "rdd_deduped = rdd_with_duration.distinct()\n",
        "\n",
        "# Method 2: Remove duplicates based on trip_id (keeps first occurrence)\n",
        "def get_trip_id_row_pair(row):\n",
        "    \"\"\"Create (trip_id, row) pair\"\"\"\n",
        "    return (row[0], row)\n",
        "\n",
        "rdd_deduped_by_id = rdd_with_duration \\\n",
        "    .map(get_trip_id_row_pair) \\\n",
        "    .reduceByKey(lambda a, b: a) \\\n",
        "    .map(lambda x: x[1])\n",
        "\n",
        "print(\"\\nDeduplicated trips:\")\n",
        "for row in rdd_deduped_by_id.collect():\n",
        "    print(row)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1bOPsnFnUMo",
        "outputId": "6cb4aa0f-320e-474e-e509-d3fa5708b018"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Deduplicated trips:\n",
            "(1, None, None, None, 15.5, None)\n",
            "(2, None, None, 'Downtown', 20.0, None)\n",
            "(3, None, None, 'Airport', None, None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Taxi Data Munging (DataFrames)\n",
        "Data munging (or data wrangling) in PySpark for taxi booking services typically involves cleaning, transforming, and enriching raw data to make it suitable for analysis. Here are some common data munging tasks with examples:\n",
        "\n",
        "### Handling Missing Values\n",
        "Taxi booking datasets often contain missing values in fields like dropoff_location, fare_amount, etc."
      ],
      "metadata": {
        "id": "QBvEM2L7cydp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "spark = SparkSession.builder.appName(\"TaxiDataMunging\").getOrCreate()\n",
        "\n",
        "# Sample Data\n",
        "data = [\n",
        "    (1, \"2025-02-18 10:00:00\", \"2025-02-18 10:30:00\", None, 15.5),\n",
        "    (2, \"2025-02-18 11:00:00\", None, \"Downtown\", 20.0),\n",
        "    (3, None, \"2025-02-18 12:00:00\", \"Airport\", None),\n",
        "]\n",
        "columns = [\"trip_id\", \"pickup_time\", \"dropoff_time\", \"dropoff_location\", \"fare_amount\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n"
      ],
      "metadata": {
        "id": "Wk0Kz0hQc1Zw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fill missing values\n",
        "In PySpark you can handle missing (null or NaN) values in a couple of ways:\n",
        "\n",
        "1. Using the DataFrameâ€™s fillna() Method\n",
        "This method lets you replace missing values with a specified constant. You can either fill every column with the same value or pass a dictionary to specify different fill values per column.\n",
        "2. Using the Imputer from pyspark.ml.feature\n",
        "For numerical columns, you might prefer to impute missing values with a statistic such as the mean or median.\n",
        "\n",
        "Both methods are commonly used depending on the nature of your data and the imputation strategy you need.\n",
        "\n",
        "fillna() / na.fill(): Use these for simple replacements where a constant or specific value per column is appropriate.\n",
        "\n",
        "Imputer: Best for numerical columns when you want a data-driven replacement (e.g., mean or median)."
      ],
      "metadata": {
        "id": "arTLiz-xXw5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned = df.fillna({\"dropoff_location\": \"Unknown\", \"fare_amount\": 0.0})\n",
        "\n",
        "df_cleaned.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCB1hgRVzjQi",
        "outputId": "6cfecfaf-f9da-43db-f792-b5b31457c0d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+-------------------+----------------+-----------+\n",
            "|trip_id|        pickup_time|       dropoff_time|dropoff_location|fare_amount|\n",
            "+-------+-------------------+-------------------+----------------+-----------+\n",
            "|      1|2025-02-18 10:00:00|2025-02-18 10:30:00|         Unknown|       15.5|\n",
            "|      2|2025-02-18 11:00:00|               NULL|        Downtown|       20.0|\n",
            "|      3|               NULL|2025-02-18 12:00:00|         Airport|        0.0|\n",
            "+-------+-------------------+-------------------+----------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Converting Datetime Formats\n",
        "Timestamps are usually in string format. We convert them to PySpark Timestamp for easy calculations. Below example converts time fields from string to timestamp for further processing.\n"
      ],
      "metadata": {
        "id": "jzo9PyDuzm0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import to_timestamp\n",
        "\n",
        "df = df.withColumn(\"pickup_time\", to_timestamp(col(\"pickup_time\")))\n",
        "df = df.withColumn(\"dropoff_time\", to_timestamp(col(\"dropoff_time\")))\n",
        "\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5nPWoEezreq",
        "outputId": "abafc98d-4562-47e6-9d4e-b9b451618461"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+-------------------+----------------+-----------+\n",
            "|trip_id|        pickup_time|       dropoff_time|dropoff_location|fare_amount|\n",
            "+-------+-------------------+-------------------+----------------+-----------+\n",
            "|      1|2025-02-18 10:00:00|2025-02-18 10:30:00|            NULL|       15.5|\n",
            "|      2|2025-02-18 11:00:00|               NULL|        Downtown|       20.0|\n",
            "|      3|               NULL|2025-02-18 12:00:00|         Airport|       NULL|\n",
            "+-------+-------------------+-------------------+----------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Filtering Invalid Data\n",
        "Remove bookings with negative fares or missing crucial data. Below example keeps only valid trips where fare is positive and pickup time is not missing.\n"
      ],
      "metadata": {
        "id": "vnUgR4JJztDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_valid = df.filter((col(\"fare_amount\") > 0) & col(\"pickup_time\").isNotNull())\n",
        "df_valid.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UzFu2cdz6Qq",
        "outputId": "4f5c8eee-051d-4d17-87dd-3506dc8d907e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+-------------------+----------------+-----------+\n",
            "|trip_id|        pickup_time|       dropoff_time|dropoff_location|fare_amount|\n",
            "+-------+-------------------+-------------------+----------------+-----------+\n",
            "|      1|2025-02-18 10:00:00|2025-02-18 10:30:00|            NULL|       15.5|\n",
            "|      2|2025-02-18 11:00:00|               NULL|        Downtown|       20.0|\n",
            "+-------+-------------------+-------------------+----------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering\n",
        "This example focuses on Trip Duration. The computation is in minutes."
      ],
      "metadata": {
        "id": "gtqpMoOOz-Q6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import unix_timestamp, col\n",
        "\n",
        "df = df.withColumn(\"trip_duration_mins\", \\\n",
        "                   (unix_timestamp(col(\"dropoff_time\")) -  \\\n",
        "                    unix_timestamp(col(\"pickup_time\"))) / 60)\n",
        "\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drz3D7PA0LuK",
        "outputId": "8ac19ed0-ab66-434e-b154-d5322317c879"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+-------------------+----------------+-----------+------------------+\n",
            "|trip_id|        pickup_time|       dropoff_time|dropoff_location|fare_amount|trip_duration_mins|\n",
            "+-------+-------------------+-------------------+----------------+-----------+------------------+\n",
            "|      1|2025-02-18 10:00:00|2025-02-18 10:30:00|            NULL|       15.5|              30.0|\n",
            "|      2|2025-02-18 11:00:00|               NULL|        Downtown|       20.0|              NULL|\n",
            "|      3|               NULL|2025-02-18 12:00:00|         Airport|       NULL|              NULL|\n",
            "+-------+-------------------+-------------------+----------------+-----------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Geospatial Filtering (Pickups in Specific Area)\n",
        "Filter trips that started from a given region (e.g., Manhattan). Selects trips that ended in Changi.\n"
      ],
      "metadata": {
        "id": "-Se92B2r0Zyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_filtered = df.filter(col(\"dropoff_location\") == \"Changi\")\n",
        "df_filtered.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiVznmUq0gc5",
        "outputId": "cc368687-7268-4832-fe81-5adc111190c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------+------------+----------------+-----------+------------------+\n",
            "|trip_id|pickup_time|dropoff_time|dropoff_location|fare_amount|trip_duration_mins|\n",
            "+-------+-----------+------------+----------------+-----------+------------------+\n",
            "+-------+-----------+------------+----------------+-----------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aggregation\n",
        "Summarize daily revenue. Calculates total fare collected per day."
      ],
      "metadata": {
        "id": "wHmOqwX70wXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import date_format, sum\n",
        "\n",
        "df_daily_fare = df.withColumn(\"date\", date_format(col(\"pickup_time\"), \"yyyy-MM-dd\")) \\\n",
        "                  .groupBy(\"date\").agg(sum(\"fare_amount\").alias(\"total_fare\"))\n",
        "\n",
        "df_daily_fare.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyR-o7E_1IqA",
        "outputId": "abe059cb-e96b-4bfc-ab3d-3d1c287b3bac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+\n",
            "|      date|total_fare|\n",
            "+----------+----------+\n",
            "|2025-02-18|      35.5|\n",
            "|      NULL|      NULL|\n",
            "+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Deduplication\n",
        "\n"
      ],
      "metadata": {
        "id": "E_ESI-S-1Oko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_deduped = df.dropDuplicates([\"trip_id\"])\n",
        "df_deduped.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gj2md2AI1WMw",
        "outputId": "dd28634a-fe07-469b-c58c-f371cc4aecf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+-------------------+----------------+-----------+------------------+\n",
            "|trip_id|        pickup_time|       dropoff_time|dropoff_location|fare_amount|trip_duration_mins|\n",
            "+-------+-------------------+-------------------+----------------+-----------+------------------+\n",
            "|      1|2025-02-18 10:00:00|2025-02-18 10:30:00|            NULL|       15.5|              30.0|\n",
            "|      2|2025-02-18 11:00:00|               NULL|        Downtown|       20.0|              NULL|\n",
            "|      3|               NULL|2025-02-18 12:00:00|         Airport|       NULL|              NULL|\n",
            "+-------+-------------------+-------------------+----------------+-----------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bucketing Fare Amounts\n",
        "Categorize trips based on fare price. Groups fares into Low, Medium, and High categories.\n"
      ],
      "metadata": {
        "id": "JIIZatI71c9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when\n",
        "df = df.withColumn(\"fare_category\", when(col(\"fare_amount\") < 10, \"Low Fare\") \\\n",
        "       .when((col(\"fare_amount\") >= 10) & (col(\"fare_amount\") < 30),  \\\n",
        "             \"Medium Fare\").otherwise(\"High Fare\"))\n",
        "\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQyHEZ4N1hLQ",
        "outputId": "60ed61b0-048b-4bf2-9f8b-c1ba06c65916"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+-------------------+----------------+-----------+------------------+-------------+\n",
            "|trip_id|        pickup_time|       dropoff_time|dropoff_location|fare_amount|trip_duration_mins|fare_category|\n",
            "+-------+-------------------+-------------------+----------------+-----------+------------------+-------------+\n",
            "|      1|2025-02-18 10:00:00|2025-02-18 10:30:00|            NULL|       15.5|              30.0|  Medium Fare|\n",
            "|      2|2025-02-18 11:00:00|               NULL|        Downtown|       20.0|              NULL|  Medium Fare|\n",
            "|      3|               NULL|2025-02-18 12:00:00|         Airport|       NULL|              NULL|    High Fare|\n",
            "+-------+-------------------+-------------------+----------------+-----------+------------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Join Data\n",
        "Merge booking details with customer data."
      ],
      "metadata": {
        "id": "HIUQGBH614eY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customer_data = [(1, \"Alice\"), (2, \"Bob\"), (3, \"Charlie\")]\n",
        "customer_df = spark.createDataFrame(customer_data, [\"customer_id\", \"customer_name\"])\n",
        "\n",
        "trip_data = [(1, 1, 20.5), (2, 2, 15.0), (3, 1, 30.0)]\n",
        "trip_df = spark.createDataFrame(trip_data, [\"trip_id\", \"customer_id\", \"fare_amount\"])\n",
        "\n",
        "df_joined = trip_df.join(customer_df, \"customer_id\", \"inner\")\n",
        "df_joined.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2w2jXyb16XA",
        "outputId": "e1617328-d025-4d4b-cfa2-ecf03462d547"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------+-----------+-------------+\n",
            "|customer_id|trip_id|fare_amount|customer_name|\n",
            "+-----------+-------+-----------+-------------+\n",
            "|          1|      1|       20.5|        Alice|\n",
            "|          1|      3|       30.0|        Alice|\n",
            "|          2|      2|       15.0|          Bob|\n",
            "+-----------+-------+-----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save Data\n",
        "Save the cleaned dataset as Parquet for efficient querying. Stores processed data in Parquet format."
      ],
      "metadata": {
        "id": "cSfUe7pS16zX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned.write.mode(\"overwrite\").parquet(\"output/taxi_cleaned.parquet\")"
      ],
      "metadata": {
        "id": "64HhzTKu2AG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Thoughts\n",
        "These PySpark data munging techniques help clean, filter, and transform raw taxi booking data, making it ready for analytics and reporting.\n",
        "Would you like an end-to-end PySpark pipeline for this? ðŸš€\n",
        "\n",
        "Yes, you can install Python and PyCharm on a Windows machine and run PySpark code within a Podman Kubernetes cluster.\n"
      ],
      "metadata": {
        "id": "9u3Viw671AaQ"
      }
    }
  ]
}