{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQ1DB4xSxBWgPsNC65NG4N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suriarasai/BEAD2026/blob/main/colab/03a_Apache_Spark_User_Defined_Functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to  PySpark User Defined Functions\n",
        "In this demo we will learn to use functional programming principles while creating effective user defined functions.\n",
        "\n",
        "## Core Concept Introduction\n",
        "While performing distributed computing with PySpark, UDF's thrive to be scalable but avoiding state share amoung worker nodes.\n",
        "1. Recall our discussions, pure functions (no side effects, no state) can run safely on any worker node.\n",
        "2. Stateful functions cause problems because state isn't shared across worker nodes.\n",
        "\n",
        "## Functions with State (Bad practice)\n",
        "### Example of Accumulator Anti Pattern\n",
        "Usage of external states are discouraged."
      ],
      "metadata": {
        "id": "wLKodrYV6TI6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import IntegerType, StringType"
      ],
      "metadata": {
        "id": "hx_RjiDf8zsw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kJQpPHO_56r_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "077819c8-8ba7-4a3c-c830-dff63e83a267"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------+\n",
            "|value|doubled|\n",
            "+-----+-------+\n",
            "|    1|      2|\n",
            "|    2|      4|\n",
            "|    3|      6|\n",
            "|    4|      8|\n",
            "|    5|     10|\n",
            "+-----+-------+\n",
            "\n",
            "Counter value: 0\n"
          ]
        }
      ],
      "source": [
        "spark = SparkSession.builder.appName(\"FunctionalDemo\").getOrCreate()\n",
        "\n",
        "# BAD: Function with external state\n",
        "counter = 0  # This won't work as expected in distributed setting\n",
        "\n",
        "def count_processing_bad(value):\n",
        "    global counter\n",
        "    counter += 1  # Trying to maintain state\n",
        "    return value * 2\n",
        "\n",
        "# Create sample data\n",
        "data = [(1,), (2,), (3,), (4,), (5,)]\n",
        "df = spark.createDataFrame(data, [\"value\"])\n",
        "\n",
        "# This will NOT work correctly in distributed mode\n",
        "count_udf_bad = udf(count_processing_bad, IntegerType())\n",
        "result_df = df.withColumn(\"doubled\", count_udf_bad(\"value\"))\n",
        "result_df.show()\n",
        "\n",
        "print(f\"Counter value: {counter}\")  # Will be 0 or unpredictable!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example of a Class with Mutable State\n",
        "Causes in consistency"
      ],
      "metadata": {
        "id": "teHDK1e67eQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BAD: Class with mutable state\n",
        "class DataProcessor:\n",
        "    def __init__(self):\n",
        "        self.processed_count = 0\n",
        "        self.sum_total = 0\n",
        "\n",
        "    def process(self, value):\n",
        "        self.processed_count += 1  # State mutation\n",
        "        self.sum_total += value    # State mutation\n",
        "        return value * 2\n",
        "\n",
        "# This won't work in distributed setting\n",
        "processor = DataProcessor()\n",
        "process_udf = udf(lambda x: processor.process(x), IntegerType())\n",
        "\n",
        "# The state will be lost/inconsistent\n",
        "result_df = df.withColumn(\"processed\", process_udf(\"value\"))\n",
        "result_df.show()\n",
        "print(f\"Processed: {processor.processed_count}\")  # Wrong!"
      ],
      "metadata": {
        "id": "arN2Rrmr7p_o",
        "outputId": "8d152596-5609-4cba-8a6f-0dd6469c1aed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---------+\n",
            "|value|processed|\n",
            "+-----+---------+\n",
            "|    1|        2|\n",
            "|    2|        4|\n",
            "|    3|        6|\n",
            "|    4|        8|\n",
            "|    5|       10|\n",
            "+-----+---------+\n",
            "\n",
            "Processed: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pure Functions (Scalable Approach)\n",
        "To create a PySpark UDF using a pure function, you wrap the function with the udf() transformation and specify the return type, which allows the function to be applied to DataFrame columns in a distributed manner.\n",
        "\n",
        "### Example of simple UDF"
      ],
      "metadata": {
        "id": "QwIhV3dc78gt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. The Pure Function\n",
        "def my_func(x): return x.upper()\n",
        "\n",
        "# 2. Creating the UDF\n",
        "upper_udf = udf(my_func, StringType())\n",
        "\n",
        "# 3. Usage\n",
        "df.withColumn(\"new_col\", upper_udf(df[\"value\"]))"
      ],
      "metadata": {
        "id": "QL1kaquU8MpC",
        "outputId": "16eaf8ba-7259-49eb-b107-7c9077ded28e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[value: bigint, new_col: string]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example of UDF with Decorators"
      ],
      "metadata": {
        "id": "vgka9XlC9XCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The decorator handles the conversion automatically\n",
        "@udf(returnType=StringType())\n",
        "def my_pure_function(text):\n",
        "    if text is None:\n",
        "        return None\n",
        "    return text.upper()\n",
        "\n",
        "# Usage (using 'value' as per your previous error)\n",
        "df_result = df.withColumn(\"new_col\", my_pure_function(\"value\"))"
      ],
      "metadata": {
        "id": "dml8mNWm9mlQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example of Pure Transformation Functions"
      ],
      "metadata": {
        "id": "Fk2xAd0r9tuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, lit, sum as spark_sum\n",
        "\n",
        "# GOOD: Pure function with no side effects\n",
        "def multiply_pure(value, factor=2):\n",
        "    \"\"\"Pure function: output depends only on input\"\"\"\n",
        "    return value * factor\n",
        "\n",
        "# Register as UDF\n",
        "multiply_udf = udf(lambda x: multiply_pure(x, 2), IntegerType())\n",
        "\n",
        "# Use the pure function\n",
        "result_df = df.withColumn(\"doubled\", multiply_udf(\"value\"))\n",
        "result_df.show()\n",
        "\n",
        "# If you need counting, use Spark's built-in aggregations\n",
        "count_result = result_df.count()\n",
        "print(f\"Total processed: {count_result}\")"
      ],
      "metadata": {
        "id": "u6v0AP2K9zVU",
        "outputId": "283d4fe9-7430-4063-bdd1-f6a21d71c00c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------+\n",
            "|value|doubled|\n",
            "+-----+-------+\n",
            "|    1|      2|\n",
            "|    2|      4|\n",
            "|    3|      6|\n",
            "|    4|      8|\n",
            "|    5|     10|\n",
            "+-----+-------+\n",
            "\n",
            "Total processed: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example if Complex Pure Functios with Multiple Inputs\n",
        "(TO BE DISCUSSED AFTER SQL LESSONS)"
      ],
      "metadata": {
        "id": "CaLczlsb-l09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, FloatType\n",
        "import json\n",
        "\n",
        "# GOOD: Pure function for complex calculations\n",
        "def calculate_metrics_pure(value, threshold, multiplier):\n",
        "    \"\"\"\n",
        "    Pure function: No external dependencies or state\n",
        "    All needed data passed as parameters\n",
        "    \"\"\"\n",
        "    adjusted = value * multiplier\n",
        "    is_above_threshold = adjusted > threshold\n",
        "    return {\n",
        "        'adjusted_value': adjusted,\n",
        "        'above_threshold': is_above_threshold,\n",
        "        'distance_from_threshold': adjusted - threshold\n",
        "    }\n",
        "\n",
        "# Create more complex dataset\n",
        "complex_data = spark.createDataFrame(\n",
        "    [(10, 25, 2.0), (20, 25, 1.5), (15, 25, 2.0)],\n",
        "    [\"value\", \"threshold\", \"multiplier\"]\n",
        ")\n",
        "\n",
        "# Define return schema\n",
        "return_schema = StructType([\n",
        "    StructField(\"adjusted_value\", FloatType()),\n",
        "    StructField(\"above_threshold\", FloatType()),\n",
        "    StructField(\"distance_from_threshold\", FloatType())\n",
        "])\n",
        "\n",
        "# Create UDF with pure function\n",
        "metrics_udf = udf(\n",
        "    lambda v, t, m: calculate_metrics_pure(v, t, m),\n",
        "    return_schema\n",
        ")\n",
        "\n",
        "# Apply the pure function\n",
        "result_df = complex_data.withColumn(\n",
        "    \"metrics\",\n",
        "    metrics_udf(col(\"value\"), col(\"threshold\"), col(\"multiplier\"))\n",
        ")\n",
        "\n",
        "result_df.select(\"value\", \"metrics.*\").show()"
      ],
      "metadata": {
        "id": "31ano_2x-kyI",
        "outputId": "24d05715-c734-424a-ffeb-8861197b3559",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------+---------------+-----------------------+\n",
            "|value|adjusted_value|above_threshold|distance_from_threshold|\n",
            "+-----+--------------+---------------+-----------------------+\n",
            "|   10|          20.0|           NULL|                   -5.0|\n",
            "|   20|          30.0|           NULL|                    5.0|\n",
            "|   15|          30.0|           NULL|                    5.0|\n",
            "+-----+--------------+---------------+-----------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example of Function Composition"
      ],
      "metadata": {
        "id": "rY_Iptbd_IEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GOOD: Composing pure functions\n",
        "def normalize(value, min_val, max_val):\n",
        "    \"\"\"Pure normalization function\"\"\"\n",
        "    return (value - min_val) / (max_val - min_val) if max_val > min_val else 0\n",
        "\n",
        "def apply_sigmoid(value):\n",
        "    \"\"\"Pure sigmoid transformation\"\"\"\n",
        "    import math\n",
        "    return 1 / (1 + math.exp(-value))\n",
        "\n",
        "def transform_pipeline(value, min_val, max_val):\n",
        "    \"\"\"Compose pure functions\"\"\"\n",
        "    normalized = normalize(value, min_val, max_val)\n",
        "    return apply_sigmoid(normalized)\n",
        "\n",
        "# Use with PySpark\n",
        "transform_udf = udf(\n",
        "    lambda x: transform_pipeline(x, 0, 100),\n",
        "    FloatType()\n",
        ")\n",
        "\n",
        "data_range = spark.range(0, 100, 10).toDF(\"value\")\n",
        "transformed_df = data_range.withColumn(\"transformed\", transform_udf(\"value\"))\n",
        "transformed_df.show()"
      ],
      "metadata": {
        "id": "p1A3dE2c_No9",
        "outputId": "62849b49-fe73-4c0e-dc30-ebe93dff6b00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----------+\n",
            "|value|transformed|\n",
            "+-----+-----------+\n",
            "|    0|        0.5|\n",
            "|   10|  0.5249792|\n",
            "|   20|   0.549834|\n",
            "|   30|  0.5744425|\n",
            "|   40| 0.59868765|\n",
            "|   50| 0.62245935|\n",
            "|   60|  0.6456563|\n",
            "|   70|  0.6681878|\n",
            "|   80|  0.6899745|\n",
            "|   90|  0.7109495|\n",
            "+-----+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handling State via PySPark Accumulators\n",
        "When we genuinely need state-like behavior, we use Spark's built-in mechanisms.\n",
        "\n",
        "### Example use of Accumulators"
      ],
      "metadata": {
        "id": "zSMCZQAg_dDu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GOOD: Using Spark's accumulator for distributed counting\n",
        "from pyspark import AccumulatorParam\n",
        "\n",
        "# Create an accumulator\n",
        "processed_counter = spark.sparkContext.accumulator(0)\n",
        "\n",
        "def process_with_accumulator(value):\n",
        "    processed_counter.add(1)  # Thread-safe accumulation\n",
        "    return value * 2\n",
        "\n",
        "# Note: Accumulators in transformations can be tricky\n",
        "# Better to use actions\n",
        "rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5])\n",
        "result = rdd.map(lambda x: process_with_accumulator(x)).collect()\n",
        "\n",
        "print(f\"Processed count: {processed_counter.value}\")"
      ],
      "metadata": {
        "id": "ySbwYHr2_r1Q",
        "outputId": "4dfd7a63-4ec0-4cc9-a714-4602618d1b01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed count: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example of Prefering Aggregations over State\n",
        "(TO BE DISCUSSED AFTER SQL LESSONS)"
      ],
      "metadata": {
        "id": "F_YwS3Nk_yJK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Window\n",
        "from pyspark.sql.functions import row_number, sum, avg, max\n",
        "\n",
        "# GOOD: Use window functions instead of stateful processing\n",
        "data_with_groups = spark.createDataFrame(\n",
        "    [(\"A\", 10), (\"A\", 20), (\"B\", 15), (\"B\", 25), (\"A\", 30)],\n",
        "    [\"group\", \"value\"]\n",
        ")\n",
        "\n",
        "# Instead of maintaining running totals in state, use window functions\n",
        "window_spec = Window.partitionBy(\"group\").orderBy(\"value\")\n",
        "\n",
        "result_df = data_with_groups.withColumn(\n",
        "    \"running_sum\", spark_sum(\"value\").over(window_spec)\n",
        ").withColumn(\n",
        "    \"running_avg\", avg(\"value\").over(window_spec)\n",
        ").withColumn(\n",
        "    \"rank\", row_number().over(window_spec)\n",
        ")\n",
        "\n",
        "result_df.show()"
      ],
      "metadata": {
        "id": "Tm4LsWMd_5sD",
        "outputId": "804928da-1db7-482d-da6b-39586460a629",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-----------+-----------+----+\n",
            "|group|value|running_sum|running_avg|rank|\n",
            "+-----+-----+-----------+-----------+----+\n",
            "|    A|   10|         10|       10.0|   1|\n",
            "|    A|   20|         30|       15.0|   2|\n",
            "|    A|   30|         60|       20.0|   3|\n",
            "|    B|   15|         15|       15.0|   1|\n",
            "|    B|   25|         40|       20.0|   2|\n",
            "+-----+-----+-----------+-----------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary and Key Takeaways\n",
        "\n",
        "1. Determinism: Pure functions always produce the same output for the same input\n",
        "2. Parallelization: Pure functions can run on any worker without coordination\n",
        "3. Testability: Pure functions are easy to unit test\n",
        "4. Debugging: Pure functions are easier to debug (no hidden state)\n",
        "5. Caching: Results of pure functions can be safely cached"
      ],
      "metadata": {
        "id": "HyImRQxBAQK0"
      }
    }
  ]
}