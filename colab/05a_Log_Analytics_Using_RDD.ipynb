{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyODewFIskDG5VQXr2KJ8cXS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suriarasai/BEAD2025/blob/main/colab/05a_Log_Analytics_Using_RDD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Scenario and Data Set\n",
        "\n",
        "We will process the famous NASA's web server logs from July 1995. Each line in the log file represents a request made to the server and follows the Common Log Format.\n",
        "\n",
        "Our Goal:\n",
        "\n",
        "Read the raw log file into an RDD.\n",
        "\n",
        "Parse each line to extract the HTTP status code (e.g., 200 for OK, 404 for Not Found) and the request URL.\n",
        "\n",
        "Filter out any malformed log entries that don't parse correctly.\n",
        "\n",
        "Count the number of times each HTTP status code appears in the entire log."
      ],
      "metadata": {
        "id": "nVafmqmmAAN2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup: Getting the Data\n",
        "\n",
        "We need to download the public dataset. We can can run the following command in code using bash operator to fetch the data."
      ],
      "metadata": {
        "id": "2wyl9kAnALF9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qr5qos81_VeX",
        "outputId": "804ef2d4-7e30-4959-a4b3-48840cc25cbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-08-16 03:52:49--  ftp://ita.ee.lbl.gov/traces/NASA_access_log_Jul95.gz\n",
            "           => ‘NASA_access_log_Jul95.gz’\n",
            "Resolving ita.ee.lbl.gov (ita.ee.lbl.gov)... 131.243.2.164, 2620:83:8000:102::a4\n",
            "Connecting to ita.ee.lbl.gov (ita.ee.lbl.gov)|131.243.2.164|:21... connected.\n",
            "Logging in as anonymous ... Logged in!\n",
            "==> SYST ... done.    ==> PWD ... done.\n",
            "==> TYPE I ... done.  ==> CWD (1) /traces ... done.\n",
            "==> SIZE NASA_access_log_Jul95.gz ... 20676672\n",
            "==> PASV ... done.    ==> RETR NASA_access_log_Jul95.gz ... done.\n",
            "Length: 20676672 (20M) (unauthoritative)\n",
            "\n",
            "NASA_access_log_Jul 100%[===================>]  19.72M  7.16MB/s    in 2.8s    \n",
            "\n",
            "2025-08-16 03:52:53 (7.16 MB/s) - ‘NASA_access_log_Jul95.gz’ saved [20676672]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# This command downloads the compressed log file and unzips it.\n",
        "!wget ftp://ita.ee.lbl.gov/traces/NASA_access_log_Jul95.gz -O NASA_access_log_Jul95.gz\n",
        "!gunzip NASA_access_log_Jul95.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will create a file named NASA_access_log_Jul95 in the workspace."
      ],
      "metadata": {
        "id": "AjFXHyvcAc2Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PySpark Setup\n",
        "\n",
        "The first step involves installing pyspark.  The next step is to install findspark library.\n",
        "\n",
        "*Note: the --ignore-install flag is used to ignore previous installations and use the latest one built alongside the allocated cluster.*\n"
      ],
      "metadata": {
        "id": "26ugKWTTdmm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# 1. Install OpenJDK 21 (if not already done in a previous cell)\n",
        "!apt-get update -qq\n",
        "!apt-get install -qq openjdk-21-jdk-headless\n",
        "\n",
        "# 2. Verify where it landed (if needed)\n",
        "!ls /usr/lib/jvm | grep 21\n",
        "\n",
        "# 3. Point to JDK 21\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-21-openjdk-amd64\"\n",
        "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
        "\n",
        "# 4. Install PySpark via pip (make sure this happens AFTER setting JAVA_HOME)\n",
        "!pip install pyspark --quiet\n",
        "\n",
        "# 5. Import and start Spark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = (\n",
        "    SparkSession.builder\n",
        "      .master(\"local[*]\")\n",
        "      .appName(\"LOg Analytics Spark on Java21\")\n",
        "      .getOrCreate()\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxU-lspxBKl7",
        "outputId": "5bec3fc7-8972-420d-f962-282132130e3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Selecting previously unselected package openjdk-21-jre-headless:amd64.\n",
            "(Reading database ... 126380 files and directories currently installed.)\n",
            "Preparing to unpack .../openjdk-21-jre-headless_21.0.8+9~us1-0ubuntu1~22.04.1_amd64.deb ...\n",
            "Unpacking openjdk-21-jre-headless:amd64 (21.0.8+9~us1-0ubuntu1~22.04.1) ...\n",
            "Selecting previously unselected package openjdk-21-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-21-jdk-headless_21.0.8+9~us1-0ubuntu1~22.04.1_amd64.deb ...\n",
            "Unpacking openjdk-21-jdk-headless:amd64 (21.0.8+9~us1-0ubuntu1~22.04.1) ...\n",
            "Setting up openjdk-21-jre-headless:amd64 (21.0.8+9~us1-0ubuntu1~22.04.1) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/java to provide /usr/bin/java (java) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jpackage to provide /usr/bin/jpackage (jpackage) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/keytool to provide /usr/bin/keytool (keytool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/rmiregistry to provide /usr/bin/rmiregistry (rmiregistry) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/lib/jexec to provide /usr/bin/jexec (jexec) in auto mode\n",
            "Setting up openjdk-21-jdk-headless:amd64 (21.0.8+9~us1-0ubuntu1~22.04.1) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jar to provide /usr/bin/jar (jar) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jarsigner to provide /usr/bin/jarsigner (jarsigner) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/javac to provide /usr/bin/javac (javac) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/javadoc to provide /usr/bin/javadoc (javadoc) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/javap to provide /usr/bin/javap (javap) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jcmd to provide /usr/bin/jcmd (jcmd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jdb to provide /usr/bin/jdb (jdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jdeprscan to provide /usr/bin/jdeprscan (jdeprscan) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jdeps to provide /usr/bin/jdeps (jdeps) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jfr to provide /usr/bin/jfr (jfr) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jimage to provide /usr/bin/jimage (jimage) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jinfo to provide /usr/bin/jinfo (jinfo) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jlink to provide /usr/bin/jlink (jlink) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jmap to provide /usr/bin/jmap (jmap) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jmod to provide /usr/bin/jmod (jmod) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jps to provide /usr/bin/jps (jps) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jrunscript to provide /usr/bin/jrunscript (jrunscript) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jshell to provide /usr/bin/jshell (jshell) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jstack to provide /usr/bin/jstack (jstack) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jstat to provide /usr/bin/jstat (jstat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jstatd to provide /usr/bin/jstatd (jstatd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jwebserver to provide /usr/bin/jwebserver (jwebserver) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/serialver to provide /usr/bin/serialver (serialver) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jhsdb to provide /usr/bin/jhsdb (jhsdb) in auto mode\n",
            "java-1.21.0-openjdk-amd64\n",
            "java-21-openjdk-amd64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In PySpark, a Spark Session is created using the SparkSession.builder method. Here's an example:"
      ],
      "metadata": {
        "id": "azsuxlr3BROJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "# import collections\n",
        "spark = SparkSession.builder.master(\"local\").appName(\"Log Analytics\").getOrCreate()"
      ],
      "metadata": {
        "id": "wHfWPvhyBSZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Log Processing\n",
        "\n",
        "Set the Regular Expression Pattern"
      ],
      "metadata": {
        "id": "VaQgO-oDAsby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "# A regular expression to parse the Common Log Format.\n",
        "# Example: 199.72.81.55 - - [01/Jul/1995:00:00:01 -0400] \"GET /history/apollo/ HTTP/1.0\" 200 6245\n",
        "LOG_PATTERN = r'^(\\S+) (\\S+) (\\S+) \\[([\\w:/]+\\s[+\\-]\\d{4})\\] \"(\\S+) (\\S+)\\s*(\\S*)\" (\\d{3}) (\\S+)'"
      ],
      "metadata": {
        "id": "-5vZABWcBd7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to parse a log line. Returns a tuple or None if parsing fails."
      ],
      "metadata": {
        "id": "YfsUX42ZBfOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_log_line(line):\n",
        "    match = re.search(LOG_PATTERN, line)\n",
        "    if match:\n",
        "        # We are interested in the status code (group 8) and the URL (group 6)\n",
        "        status_code = int(match.group(8))\n",
        "        url = match.group(6)\n",
        "        return (status_code, url)\n",
        "    else:\n",
        "        return None"
      ],
      "metadata": {
        "id": "wXJOts5SBiwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read the text file into an RDD"
      ],
      "metadata": {
        "id": "EDPI76b4BlUQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read Log File\n",
        "log_file_path = \"/content/NASA_access_log_Jul95\"\n",
        "# Each line of the file becomes an element in the RDD.\n",
        "log_rdd = spark.sparkContext.textFile(log_file_path)"
      ],
      "metadata": {
        "id": "BnJI3rObBtJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Munging"
      ],
      "metadata": {
        "id": "nk2A8IzwBzgy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parsed_logs_rdd = log_rdd.map(parse_log_line)\n",
        "# Filter out the lines that failed to parse (returned None)\n",
        "valid_logs_rdd = parsed_logs_rdd.filter(lambda x: x is not None)\n",
        "\n",
        "# The RDD is now structured as (status_code, url).\n",
        "# For our goal, we just need the status code.\n",
        "# map() -> (200, 1), (404, 1), (200, 1), ...\n",
        "status_counts_rdd = valid_logs_rdd.map(lambda x: (x[0], 1))"
      ],
      "metadata": {
        "id": "mB1tEbCUB2lO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Log Consolidation"
      ],
      "metadata": {
        "id": "hyCBahq7Camq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the occurrences of each status code\n",
        "# reduceByKey() aggregates all values for a given key.\n",
        "# For key 200, it will compute: (200, 1+1+1+...)\n",
        "status_counts = status_counts_rdd.reduceByKey(lambda x, y: x + y)"
      ],
      "metadata": {
        "id": "-aQKUfMiCSvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collect Results and Print"
      ],
      "metadata": {
        "id": "Q4D0tC2FCiMl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Collect and Print Results\n",
        "# Let's see the top 10 most frequent status codes\n",
        "top_10_status_codes = status_counts.takeOrdered(10, key=lambda x: -x[1])\n",
        "print(\"--- Top 10 HTTP Status Code Counts ---\")\n",
        "for status, count in top_10_status_codes:\n",
        "    print(f\"Status Code: {status}, Count: {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBLqR82rCpHq",
        "outputId": "20976d51-54c2-403d-88c0-0b03872f3ce5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Top 10 HTTP Status Code Counts ---\n",
            "Status Code: 200, Count: 1700743\n",
            "Status Code: 304, Count: 132626\n",
            "Status Code: 302, Count: 46569\n",
            "Status Code: 404, Count: 10783\n",
            "Status Code: 500, Count: 62\n",
            "Status Code: 403, Count: 54\n",
            "Status Code: 501, Count: 14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stop"
      ],
      "metadata": {
        "id": "8_U-EdfjCz1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "WueF9VvzC16O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "End of Use Case"
      ],
      "metadata": {
        "id": "UcsijUMEC5Z5"
      }
    }
  ]
}