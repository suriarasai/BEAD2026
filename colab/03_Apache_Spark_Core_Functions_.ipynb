{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1CWLfwBSfIFai4_aEkQ7qYT8H5ENZIN0O",
      "authorship_tag": "ABX9TyP5anuBzQ0yaURc+swza6x1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suriarasai/BEAD2026/blob/main/colab/03_Apache_Spark_Core_Functions_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Apache PySpark\n",
        "In this demo we will see how we can run PySpark in a Google\n",
        " Colaboratory notebook. We will also perform some basic data exploratory tasks common to data science problems.\n",
        "\n"
      ],
      "metadata": {
        "id": "jaVSpe-PbxkO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PySpark Install\n",
        "\n",
        "The first step involves installing pyspark.  The next step is to install findspark library.\n",
        "\n",
        "*Note: the --ignore-install flag is used to ignore previous installations and use the latest one built alongside the allocated cluster.*\n"
      ],
      "metadata": {
        "id": "26ugKWTTdmm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# 1. Install OpenJDK 21 (if not already done in a previous cell)\n",
        "!apt-get update -qq\n",
        "!apt-get install -qq openjdk-21-jdk-headless\n",
        "\n",
        "# 2. Verify where it landed (if needed)\n",
        "!ls /usr/lib/jvm | grep 21\n",
        "\n",
        "# 3. Point to JDK 21\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-21-openjdk-amd64\"\n",
        "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
        "\n",
        "# 4. Install PySpark via pip (make sure this happens AFTER setting JAVA_HOME)\n",
        "!pip install pyspark --quiet\n",
        "\n",
        "# 5. Import and start Spark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = (\n",
        "    SparkSession.builder\n",
        "      .master(\"local[*]\")\n",
        "      .appName(\"Spark on Java21\")\n",
        "      .getOrCreate()\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "7cdc0oJjb8N5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a317f33-dd52-4b77-a0f2-e6bb31b8a601"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Selecting previously unselected package openjdk-21-jre-headless:amd64.\n",
            "(Reading database ... 117528 files and directories currently installed.)\n",
            "Preparing to unpack .../openjdk-21-jre-headless_21.0.9+10-1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-21-jre-headless:amd64 (21.0.9+10-1~22.04) ...\n",
            "Selecting previously unselected package openjdk-21-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-21-jdk-headless_21.0.9+10-1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-21-jdk-headless:amd64 (21.0.9+10-1~22.04) ...\n",
            "Setting up openjdk-21-jre-headless:amd64 (21.0.9+10-1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/java to provide /usr/bin/java (java) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jpackage to provide /usr/bin/jpackage (jpackage) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/keytool to provide /usr/bin/keytool (keytool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/rmiregistry to provide /usr/bin/rmiregistry (rmiregistry) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/lib/jexec to provide /usr/bin/jexec (jexec) in auto mode\n",
            "Setting up openjdk-21-jdk-headless:amd64 (21.0.9+10-1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jar to provide /usr/bin/jar (jar) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jarsigner to provide /usr/bin/jarsigner (jarsigner) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/javac to provide /usr/bin/javac (javac) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/javadoc to provide /usr/bin/javadoc (javadoc) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/javap to provide /usr/bin/javap (javap) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jcmd to provide /usr/bin/jcmd (jcmd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jdb to provide /usr/bin/jdb (jdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jdeprscan to provide /usr/bin/jdeprscan (jdeprscan) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jdeps to provide /usr/bin/jdeps (jdeps) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jfr to provide /usr/bin/jfr (jfr) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jimage to provide /usr/bin/jimage (jimage) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jinfo to provide /usr/bin/jinfo (jinfo) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jlink to provide /usr/bin/jlink (jlink) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jmap to provide /usr/bin/jmap (jmap) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jmod to provide /usr/bin/jmod (jmod) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jps to provide /usr/bin/jps (jps) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jrunscript to provide /usr/bin/jrunscript (jrunscript) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jshell to provide /usr/bin/jshell (jshell) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jstack to provide /usr/bin/jstack (jstack) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jstat to provide /usr/bin/jstat (jstat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jstatd to provide /usr/bin/jstatd (jstatd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jwebserver to provide /usr/bin/jwebserver (jwebserver) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/serialver to provide /usr/bin/serialver (serialver) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jhsdb to provide /usr/bin/jhsdb (jhsdb) in auto mode\n",
            "java-1.21.0-openjdk-amd64\n",
            "java-21-openjdk-amd64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spark Session\n",
        "\n",
        "We import the basic object SparkSession from the Spark Framework. In PySpark, a Spark Session is a unified entry point for reading data, configuring the system, and managing various Spark services.\n",
        "\n",
        "Here's a breakdown of what the Spark Session does:\n",
        "\n",
        "1. Unified Entry Point: It's the central point to access all Spark  functionalities, making it simpler and more intuitive to use Spark for development.\n",
        "2. Data Reading and Writing: We use the Spark Session to read data from various sources (like HDFS, S3, JDBC, Hive, etc.) and write data to various sinks.\n",
        "3. Configuration Management: It allows us to configure various aspects of the Spark application, such as setting configuration parameters.\n",
        "4. Creating DataFrames and Datasets: The Spark Session provides methods to create DataFrames and Datasets, which are the core data structures in Spark.\n",
        "5. Execution of SQL Queries: We can run SQL queries by using the Spark Session, especially when dealing with structured data.\n",
        "6. Managing Spark Services: It also helps in managing underlying Spark services like SparkContext, and it's the main point of interaction when dealing with structured data.\n",
        "\n",
        "In PySpark, a Spark Session is created using the SparkSession.builder method. Here's an example:"
      ],
      "metadata": {
        "id": "c7coAqaRcsJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "# import collections\n",
        "spark = SparkSession.builder.master(\"local\").appName(\"My App \").getOrCreate()"
      ],
      "metadata": {
        "id": "l1I1VynS4JBT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sparkContext"
      ],
      "metadata": {
        "id": "c-XvTqWhANe5",
        "outputId": "0440bba5-4ab6-4605-9a16-5c7fe784cdec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<SparkContext master=local[*] appName=Spark on Java21>"
            ],
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://7d5721387b44:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v4.0.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Spark on Java21</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Count\n",
        "To count the number of words from a file."
      ],
      "metadata": {
        "id": "ey77Nt4npScI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wc = spark.sparkContext.textFile(\"wordcount.txt\") \\\n",
        "   .flatMap(lambda line: line.split(\" \")) \\\n",
        "   .map(lambda word: (word, 1)) \\\n",
        "    .reduceByKey(lambda a, b: a + b)\n",
        "print(wc.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5SLOVbZpVju",
        "outputId": "ac6ed581-63aa-4264-e2e4-3b4a43d341de"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('wc', 1), ('text', 1), ('to', 1), ('show', 1), ('lines', 1), ('are', 1), ('This', 1), ('is', 1), ('a', 1), ('simple', 1), ('counted', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Line Count\n",
        "To count the number of lines from a file."
      ],
      "metadata": {
        "id": "Ux4bAHTnv70Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "someFile = \"wordcount.txt\"\n",
        "# the above file is under your pythonProject folder\n",
        "spark = SparkSession.builder.appName(\"SimpleApp\").getOrCreate()\n",
        "print(spark.read.text(someFile).count())\n"
      ],
      "metadata": {
        "id": "wRm36sJXv7mr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a266d67f-f697-420c-d086-453ca0495a6e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mounting Google Drive\n",
        "Connect to Google Drive"
      ],
      "metadata": {
        "id": "AVsBhvHTvokF"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddxRp3RNh1Fx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f6f51b6-52a2-4475-b2d9-54d52d5df2fb"
      },
      "source": [
        "# to read in data from a text file, first upload the data file into your google drive and then mount your google drive onto colab\n",
        "from google.colab import drive\n",
        "# to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True)\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "someFile = \"/content/drive/MyDrive/data/Customer.csv\"\n",
        "# the above file is under your pythonProject folder\n",
        "spark = SparkSession.builder.appName(\"SimpleApp\").getOrCreate()\n",
        "print(spark.read.text(someFile).count())\n"
      ],
      "metadata": {
        "id": "9aPoi-6RvrFe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3846920-42a5-44dc-a91c-5176f9497d75"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "51\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Creation\n",
        "\n",
        "### Create a simple from spark RDD\n",
        "In this examples below we want to see how to create a simple data structure using spark core commands\n",
        "\n",
        "#### Example 1: From RDD\n",
        "To create an RDD using a SparkSession in PySpark, you first need to initialize a SparkSession and then use it to create an RDD. Here's a simple example where we'll create an RDD from a tuple of numbers using a SparkSession"
      ],
      "metadata": {
        "id": "dJWVmhPJdK6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an RDD from a list of numbers\n",
        "numbers = (1, 2, 3, 4, 5)\n",
        "numbers_rdd = spark.sparkContext.parallelize(numbers)\n",
        "# Syntax print(spark.sparkContext.parallelize(\"(A B C)\").collect())\n",
        "print(f\"using collect() function\",numbers_rdd.collect())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TW2Dw06WllqY",
        "outputId": "eb7cce39-0e64-4fe3-84d1-7b75c3bad082"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using collect() function [1, 2, 3, 4, 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Alternative is to convert to a new data structure that is print friendly\n",
        "print(f\"using pipeline of functions\")\n",
        "print(tuple(spark.sparkContext.parallelize(numbers).collect()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpBiYXVYFTeR",
        "outputId": "c2749170-e1bf-45fb-87e7-354313b8b917"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using pipeline of functions\n",
            "(1, 2, 3, 4, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outfile = spark.read.text(\"/content/drive/MyDrive/data/guttenberg/AliceAdventuresInWonderland.txt\")"
      ],
      "metadata": {
        "id": "7jZjqGDPF6LM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let us count the lines in the out.txt"
      ],
      "metadata": {
        "id": "R7FUs4kZGTIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(outfile.count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZU5TmvCGWk6",
        "outputId": "54d992c2-ccbd-434d-eec7-9f7ab75f85df"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3760\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example 2: From Local File\n",
        "\n",
        "To create an RDD in PySpark by reading data from a CSV file, such as \"customers.csv\", you'll use a SparkSession to read the CSV and then convert the DataFrame to an RDD. Here's a step-by-step example:\n",
        "\n",
        "1. Initialize a SparkSession. (Done)\n",
        "2. Read the \"customers.csv\" file into a DataFrame.\n",
        "3. Convert the DataFrame to an RDD.\n",
        "4. Perform a simple action on the RDD, like counting the number of records.\n",
        "\n",
        "Here's the code snippet for this process:"
      ],
      "metadata": {
        "id": "mguhdx0po7H0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customers = spark.read.csv(\"/content/drive/MyDrive/data/Customer.csv\", header=True, inferSchema=True).rdd\n",
        "\n",
        "# Perform a simple action: count the number of records\n",
        "record_count = customers.count()\n",
        "print(f\"Number of records: {record_count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmFLTKiupPZ-",
        "outputId": "438426f4-5a08-4a6c-b250-d2a7effa7fde"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of records: 50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example 2: From Local TEXT File\n",
        "\n",
        "To create an RDD in PySpark by reading data from a CSV file, such as \"customers.csv\", you'll use a SparkSession to read the CSV and then convert the DataFrame to an RDD. Here's a step-by-step example:\n",
        "\n",
        "1. Initialize a SparkSession. (Done)\n",
        "2. Read the \"airports.txt\" file into a DataFrame.\n",
        "3. Convert the DataFrame to an RDD.\n",
        "4. Perform a simple actions on this RDD latere.\n",
        "\n",
        "A text dataset is pointed to by path. The path can be either a single text file or a directory of text files."
      ],
      "metadata": {
        "id": "Vxpc9XjmFrfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "airports = spark.read.text(\"/content/drive/MyDrive/data/airport-data/airports.txt\")"
      ],
      "metadata": {
        "id": "c1bAF3KFF7OY"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Actions and Transformation\n",
        "\n",
        "In PySpark, operations on RDDs can be broadly classified into two categories: transformations and actions. Transformations create a new RDD from an existing one, while actions return a value after running a computation on the RDD. Below are simple examples demonstrating the use of transformations and actions.\n",
        "\n",
        "###Transformations\n",
        "\n",
        "####Map\n",
        "Applies a function to each element and returns a new RDD.\n"
      ],
      "metadata": {
        "id": "EL3V5wUNridx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = spark.sparkContext.parallelize((1, 2, 3, 4, 5))\n",
        "# Traditional Python map(function, collection) (few MBs - GB fails)\n",
        "# Scalabale map (Peta - support)\n",
        "# iterablecollection.map(function) -> Object\n",
        "# collect() Object to collection\n",
        "print(tuple(rdd.map(lambda x: x * x).collect()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3o9yPBOr-ss",
        "outputId": "c63c54ee-5c77-4cee-b467-9232147b6796"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 4, 9, 16, 25)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Filter\n",
        "Returns a new RDD containing only the elements that satisfy a condition."
      ],
      "metadata": {
        "id": "ytxb4hDSsMkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(rdd.filter(lambda x: x % 2 == 0).collect())  # Keeps even numbers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJKFicO-sSrR",
        "outputId": "fc83f081-4f1d-4792-caff-33b8542eb400"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####FlatMap\n",
        "Similar to map, but each input item can be mapped to 0 or more output items."
      ],
      "metadata": {
        "id": "7hK0FzEUsZQf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = spark.sparkContext.parallelize([\"hello world\", \"hi\", \"hello mars\", \"hello jupiter\", \"hello saturn\"])\n",
        "print(words.flatMap(lambda x: x.split(\" \")).collect())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BrAp7Uwskim",
        "outputId": "e023f256-5786-4bb7-fb57-d9def0aedd2c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello', 'world', 'hi', 'hello', 'mars', 'hello', 'jupiter', 'hello', 'saturn']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Distinct\n",
        "Returns a new RDD containing distinct elements from the original RDD."
      ],
      "metadata": {
        "id": "eDWTyCeks1iE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(words.flatMap(lambda x: x.split(\" \")).distinct().collect())"
      ],
      "metadata": {
        "id": "74xIHlGchlEE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8124bc18-341b-40e6-a9b2-e74882ad02c4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello', 'world', 'mars', 'hi', 'jupiter', 'saturn']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tuples = spark.sparkContext.parallelize((1, 1, 2, 3, 3, 4))\n",
        "print(tuples.distinct().collect())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGk2e8kLs6cr",
        "outputId": "680c2f22-fc1a-4bdb-d5bf-ce12dd715282"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 4, 1, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EV27CJOfIJxv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Actions\n",
        "####Collect\n",
        "Returns all the elements of the RDD as an array to the driver program."
      ],
      "metadata": {
        "id": "uO6zm2ZgtPYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tuples.distinct().collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrJG5bFCtaIy",
        "outputId": "907a47ee-1ed9-4b21-804e-b9d1b4bbd56a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 4, 1, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Count\n",
        "Returns the number of elements in the RDD."
      ],
      "metadata": {
        "id": "w9B3hqYsth7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tuples.count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yi1FyZRtlcn",
        "outputId": "78c325b3-ef2f-4438-fc29-d6846c83c9b4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Take\n",
        "Returns an array with the first n elements of the RDD."
      ],
      "metadata": {
        "id": "R6AhdGsltqht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "first_three = tuples.take(3)\n",
        "print(first_three)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_EERBI9tuQu",
        "outputId": "26d230fc-4541-4551-851e-6d7b7168028b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "customers.take(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q56QYaL7N4OH",
        "outputId": "fce21025-e2df-4cbc-c491-9bf10d8df4dc"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(CustomerID=1000, CustomerName='Lou Anna Tan', MemberCategory='A', Age=29, Gender='F', AmountSpent=4.14, Address='Blk 26, Telok Blangah Crescent #22-87, Singapore 0409', City='Frankfurt', CountryCode='GER', ContactTitle='Ms', PhoneNumber=2732287),\n",
              " Row(CustomerID=1001, CustomerName='Wong Sook Huey', MemberCategory='A', Age=37, Gender='F', AmountSpent=67.1, Address='Blk 1007 Teresa Ville Lower Delta Road #06-02, Singapore 0410', City='Singapore', CountryCode='SIN', ContactTitle='Ms', PhoneNumber=2740975)]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Reduce\n",
        "Aggregates the elements of the RDD using a function."
      ],
      "metadata": {
        "id": "Zk6Xizictyfh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sum = tuples.reduce(lambda a, b: a + b)\n",
        "print(sum)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IY7SkGfvt2-g",
        "outputId": "1b4cfaf9-68a5-4427-c7eb-b03c68112eb6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These examples illustrate basic operations in PySpark, allowing you to manipulate and analyze large datasets efficiently. To run these examples, ensure you have a SparkContext (sc) initialized in your PySpark environment.\n",
        "\n",
        "### How to pretty print in PySaprk?\n",
        "\n",
        "The take() function and iteration in PySpark will mimic the pretty print function, but use them wisely."
      ],
      "metadata": {
        "id": "fX1Ur6gyqFDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pprint()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5y1ezBHTOrAf",
        "outputId": "c4b43ed7-878d-4b74-9f2f-4d5d8ae86b3a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pretty printing has been turned OFF\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"First five records of customer data set\", customers.take(5))\n",
        "print(\"Not so pretty....\")\n",
        "print(\"Now let us pretty print:\")\n",
        "# To pretty print, you need to iterate\n",
        "for element in customers.take(10):\n",
        "    print(element)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QcNCFRZqkiL",
        "outputId": "72f1febc-50bd-4378-96ad-3bd3cd2a72e4"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First five records of customer data set [Row(CustomerID=1000, CustomerName='Lou Anna Tan', MemberCategory='A', Age=29, Gender='F', AmountSpent=4.14, Address='Blk 26, Telok Blangah Crescent #22-87, Singapore 0409', City='Frankfurt', CountryCode='GER', ContactTitle='Ms', PhoneNumber=2732287), Row(CustomerID=1001, CustomerName='Wong Sook Huey', MemberCategory='A', Age=37, Gender='F', AmountSpent=67.1, Address='Blk 1007 Teresa Ville Lower Delta Road #06-02, Singapore 0410', City='Singapore', CountryCode='SIN', ContactTitle='Ms', PhoneNumber=2740975), Row(CustomerID=1002, CustomerName='Ng Choon Seng', MemberCategory='C', Age=23, Gender='M', AmountSpent=63.18, Address='Blk 63 Bishan St 21 #06-01, Singapore 1057', City='Toronto', CountryCode='CAN', ContactTitle='Mr', PhoneNumber=2580742), Row(CustomerID=1003, CustomerName='Chew Teck Kuan', MemberCategory='C', Age=63, Gender='M', AmountSpent=64.49, Address='Blk 109 Bedok North Rd #06-2316, Singapore 1046', City='Singapore', CountryCode='SIN', ContactTitle='Mr', PhoneNumber=4434675), Row(CustomerID=1111, CustomerName='Steven Ou', MemberCategory='B', Age=61, Gender='M', AmountSpent=44.51, Address='Blk 244, Bukit Panjang Ring Road, #11-184, Singapore 2367', City='Rio ', CountryCode='BRA', ContactTitle='Mr', PhoneNumber=7620324)]\n",
            "Not so pretty....\n",
            "Now let us pretty print:\n",
            "Row(CustomerID=1000, CustomerName='Lou Anna Tan', MemberCategory='A', Age=29, Gender='F', AmountSpent=4.14, Address='Blk 26, Telok Blangah Crescent #22-87, Singapore 0409', City='Frankfurt', CountryCode='GER', ContactTitle='Ms', PhoneNumber=2732287)\n",
            "Row(CustomerID=1001, CustomerName='Wong Sook Huey', MemberCategory='A', Age=37, Gender='F', AmountSpent=67.1, Address='Blk 1007 Teresa Ville Lower Delta Road #06-02, Singapore 0410', City='Singapore', CountryCode='SIN', ContactTitle='Ms', PhoneNumber=2740975)\n",
            "Row(CustomerID=1002, CustomerName='Ng Choon Seng', MemberCategory='C', Age=23, Gender='M', AmountSpent=63.18, Address='Blk 63 Bishan St 21 #06-01, Singapore 1057', City='Toronto', CountryCode='CAN', ContactTitle='Mr', PhoneNumber=2580742)\n",
            "Row(CustomerID=1003, CustomerName='Chew Teck Kuan', MemberCategory='C', Age=63, Gender='M', AmountSpent=64.49, Address='Blk 109 Bedok North Rd #06-2316, Singapore 1046', City='Singapore', CountryCode='SIN', ContactTitle='Mr', PhoneNumber=4434675)\n",
            "Row(CustomerID=1111, CustomerName='Steven Ou', MemberCategory='B', Age=61, Gender='M', AmountSpent=44.51, Address='Blk 244, Bukit Panjang Ring Road, #11-184, Singapore 2367', City='Rio ', CountryCode='BRA', ContactTitle='Mr', PhoneNumber=7620324)\n",
            "Row(CustomerID=1634, CustomerName='Sridharan Jayanthi', MemberCategory='A', Age=55, Gender='F', AmountSpent=61.51, Address='Blk 232, Jurong East Street 21 #02-436, Singapore 1234', City='Singapore', CountryCode='SIN', ContactTitle='Ms', PhoneNumber=6658037)\n",
            "Row(CustomerID=1681, CustomerName='Terence Lim', MemberCategory='C', Age=30, Gender='M', AmountSpent=59.62, Address='Blk 99, Balestier Road, #12-168, Singapore 1232', City='Columbo', CountryCode='SLR', ContactTitle='Mr', PhoneNumber=3551385)\n",
            "Row(CustomerID=1810, CustomerName='Vanessa Ong', MemberCategory='C', Age=32, Gender='F', AmountSpent=80.97, Address='Blk 20, Eunos Crescent, #04-2965, Singapore 1400', City='Singapore', CountryCode='SIN', ContactTitle='Ms', PhoneNumber=7487923)\n",
            "Row(CustomerID=1811, CustomerName='Koh Ting Ting', MemberCategory='B', Age=57, Gender='F', AmountSpent=21.52, Address='Blk 61, Upper Paya Lebar Road, Singapore 1953', City='Singapore', CountryCode='SIN', ContactTitle='Ms', PhoneNumber=2827208)\n",
            "Row(CustomerID=1818, CustomerName='Chionh Choon Lee', MemberCategory='A', Age=57, Gender='M', AmountSpent=7.13, Address='Blk 89, Zion Road, #16-137, Singapore 0316', City='Singapore', CountryCode='SIN', ContactTitle='Mr', PhoneNumber=7333100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Key Operations\n",
        "\n",
        "PySpark examples for key based functions are groupByKey, reduceByKey, and sortByKey operations. Let us look at how they work.\n",
        "\n",
        "####groupByKey\n",
        "This operation groups the values for each key in the RDD into a single sequence.\n",
        "####reduceByKey\n",
        "This operation merges the values for each key using an associative reduce function.\n",
        "####sortByKey\n",
        "This operation sorts the dataset by keys.\n",
        "\n",
        "Let us put together an example to compare and contrast\n"
      ],
      "metadata": {
        "id": "S8HxXONGuoxA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = spark.sparkContext.parallelize([(3, 6),(1, 2),(3, 4)])\n",
        "grouped = rdd.groupByKey()\n",
        "for key, values in grouped.collect():\n",
        "    print(f\"{key}: {tuple(values)}\")\n",
        "reduced = rdd.reduceByKey(lambda a, b: a + b)\n",
        "print(reduced.collect())\n",
        "sorted_rdd = rdd.sortByKey()\n",
        "print(sorted_rdd.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cf_-cHMhvD0V",
        "outputId": "aaeba09a-db38-4289-9794-226031ec5560"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3: (6, 4)\n",
            "1: (2,)\n",
            "[(3, 10), (1, 2)]\n",
            "[(1, 2), (3, 6), (3, 4)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please note that these operations are transformations and require an action like collect to retrieve the data. Also, keep in mind that groupByKey can cause a lot of data shuffling over the network, and it's generally more efficient to use reduceByKey where possible because it combines output values locally before sending data over the network."
      ],
      "metadata": {
        "id": "SETKMpJDwAZK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sampling\n",
        "\n",
        " The sample() transformation is used to sample a fraction of the data from an RDD. You can sample with or without replacement. Here's how you can use it:\n",
        "\n",
        "####Sampling without replacement\n"
      ],
      "metadata": {
        "id": "3-XSy6Lqwt4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, use the sample function to take a random sample of about 10% of the customers without replacement\n",
        "sampled_customers_rdd = customers.sample(False, 0.2)\n",
        "\n",
        "# Collect the results\n",
        "sampled_customers = sampled_customers_rdd.collect()\n",
        "\n",
        "# Print the sampled list of customers\n",
        "for customer in sampled_customers:\n",
        "    print(customer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rfRqPNOw3Ft",
        "outputId": "e9f9c311-624e-4813-d30d-b4b548eebfc2"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row(CustomerID=1001, CustomerName='Wong Sook Huey', MemberCategory='A', Age=37, Gender='F', AmountSpent=67.1, Address='Blk 1007 Teresa Ville Lower Delta Road #06-02, Singapore 0410', City='Singapore', CountryCode='SIN', ContactTitle='Ms', PhoneNumber=2740975)\n",
            "Row(CustomerID=1111, CustomerName='Steven Ou', MemberCategory='B', Age=61, Gender='M', AmountSpent=44.51, Address='Blk 244, Bukit Panjang Ring Road, #11-184, Singapore 2367', City='Rio ', CountryCode='BRA', ContactTitle='Mr', PhoneNumber=7620324)\n",
            "Row(CustomerID=2233, CustomerName='Too Siew Hong', MemberCategory='B', Age=35, Gender='F', AmountSpent=21.83, Address='Blk 749, Pasir Ris St 71, #09-66, Singapore 1651', City='Singapore', CountryCode='SIN', ContactTitle='Ms', PhoneNumber=5847682)\n",
            "Row(CustomerID=2345, CustomerName='Ng Teck Kie Anthony', MemberCategory='A', Age=56, Gender='M', AmountSpent=73.93, Address='Blk 105, Gangsa Road, #02-103, Singapore 2367', City='Singapore', CountryCode='SIN', ContactTitle='Mr', PhoneNumber=7690237)\n",
            "Row(CustomerID=2820, CustomerName='Ng Wee Hock John', MemberCategory='A', Age=56, Gender='M', AmountSpent=66.22, Address='Blk 417, Woodlands St 41, #02-143, Singapore 2573', City='Singapore', CountryCode='SIN', ContactTitle='Mr', PhoneNumber=3622055)\n",
            "Row(CustomerID=6969, CustomerName='Jason Young', MemberCategory='C', Age=52, Gender='M', AmountSpent=15.85, Address='blk 48 dorset rd #24-111 singapore 0821', City='Singapore', CountryCode='SIN', ContactTitle='Mr', PhoneNumber=6969696)\n",
            "Row(CustomerID=7616, CustomerName='Cheong Pei Sian', MemberCategory='B', Age=57, Gender='F', AmountSpent=92.57, Address='8 Westwood Walk, Singapore 1234', City='Singapore', CountryCode='SIN', ContactTitle='Ms', PhoneNumber=7918584)\n",
            "Row(CustomerID=8888, CustomerName='Kelvin Koh', MemberCategory='B', Age=53, Gender='M', AmountSpent=16.0, Address='Blk 25, Towner Road, #04-09, Singapore 2345', City='Singapore', CountryCode='SIN', ContactTitle='Mr', PhoneNumber=7789876)\n",
            "Row(CustomerID=9847, CustomerName='Low Soo Chiew', MemberCategory='B', Age=51, Gender='M', AmountSpent=33.32, Address='Blk 42, Bedok South Road, #05-747, Singapore 1646', City='Singapore', CountryCode='SIN', ContactTitle='Mr', PhoneNumber=4454331)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the sample method:\n",
        "\n",
        "1. The first argument is withReplacement. Set it to False for sampling without replacement, meaning a particular customer can be chosen only once.\n",
        "2. The second argument is the fraction of the data to sample, which is 0.1 in this case, meaning approximately 10% of the data.\n",
        "\n",
        "This will output a random sample of the customers from your customers_rdd. The collect() action is used here for demonstration purposes, and it should be used with caution if the dataset is large, as it will gather all the sampled data to the driver node.\n",
        "\n",
        "####Sampling with replacement\n",
        "\n",
        "The following example shows how to use sample() with replacement. This means an element can be included in the sample multiple times."
      ],
      "metadata": {
        "id": "TZYw0oFExAcm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, use the sample function to take a random sample of about 10% of the customers with replacement\n",
        "sampled_customers_rdd = customers.sample(True, 0.2)\n",
        "\n",
        "# Collect the results\n",
        "sampled_customers = sampled_customers_rdd.collect()\n",
        "\n",
        "# Print the sampled list of customers\n",
        "for customer in sampled_customers:\n",
        "    print(customer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzQm9EbUxEq4",
        "outputId": "aac98779-ad60-41a7-cc7b-e864ed4134dd"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row(CustomerID=1003, CustomerName='Chew Teck Kuan', MemberCategory='C', Age=63, Gender='M', AmountSpent=64.49, Address='Blk 109 Bedok North Rd #06-2316, Singapore 1046', City='Singapore', CountryCode='SIN', ContactTitle='Mr', PhoneNumber=4434675)\n",
            "Row(CustomerID=1811, CustomerName='Koh Ting Ting', MemberCategory='B', Age=57, Gender='F', AmountSpent=21.52, Address='Blk 61, Upper Paya Lebar Road, Singapore 1953', City='Singapore', CountryCode='SIN', ContactTitle='Ms', PhoneNumber=2827208)\n",
            "Row(CustomerID=1818, CustomerName='Chionh Choon Lee', MemberCategory='A', Age=57, Gender='M', AmountSpent=7.13, Address='Blk 89, Zion Road, #16-137, Singapore 0316', City='Singapore', CountryCode='SIN', ContactTitle='Mr', PhoneNumber=7333100)\n",
            "Row(CustomerID=1818, CustomerName='Chionh Choon Lee', MemberCategory='A', Age=57, Gender='M', AmountSpent=7.13, Address='Blk 89, Zion Road, #16-137, Singapore 0316', City='Singapore', CountryCode='SIN', ContactTitle='Mr', PhoneNumber=7333100)\n",
            "Row(CustomerID=1818, CustomerName='Chionh Choon Lee', MemberCategory='A', Age=57, Gender='M', AmountSpent=7.13, Address='Blk 89, Zion Road, #16-137, Singapore 0316', City='Singapore', CountryCode='SIN', ContactTitle='Mr', PhoneNumber=7333100)\n",
            "Row(CustomerID=3845, CustomerName='Tan Choon Heong', MemberCategory='A', Age=21, Gender='M', AmountSpent=10.82, Address='Blk 303, Shunfu Road, #03-59, Singapore 4567', City='Singapore', CountryCode='SIN', ContactTitle='Mr', PhoneNumber=3580709)\n",
            "Row(CustomerID=3845, CustomerName='Tan Choon Heong', MemberCategory='A', Age=21, Gender='M', AmountSpent=10.82, Address='Blk 303, Shunfu Road, #03-59, Singapore 4567', City='Singapore', CountryCode='SIN', ContactTitle='Mr', PhoneNumber=3580709)\n",
            "Row(CustomerID=4312, CustomerName='Leong Wai Fun', MemberCategory='A', Age=23, Gender='F', AmountSpent=56.25, Address='Blk 43, Hindhede Walk, #04-06, Singapore 1234', City='Geneva', CountryCode='SWZ', ContactTitle='Ms', PhoneNumber=4674116)\n",
            "Row(CustomerID=4567, CustomerName='Abdul Zaidi', MemberCategory='A', Age=29, Gender='M', AmountSpent=31.65, Address='Blk 24, Telok Blangah Crescent, #14-14, Singapore 0409', City='Singapore', CountryCode='SIN', ContactTitle='Mr', PhoneNumber=2709466)\n",
            "Row(CustomerID=5108, CustomerName='Cho Wee Weng', MemberCategory='A', Age=50, Gender='M', AmountSpent=24.8, Address='Blk 208, Toa Payoh North, #08-1271, Singapore 1234', City='Singapore', CountryCode='SIN', ContactTitle='Mr', PhoneNumber=2541126)\n",
            "Row(CustomerID=5156, CustomerName='Lee Boon Kiat', MemberCategory='A', Age=32, Gender='M', AmountSpent=73.91, Address='26A, Lengkong Satu, Singapore 1441', City='Singapore', CountryCode='SIN', ContactTitle='Mr', PhoneNumber=4423368)\n",
            "Row(CustomerID=6969, CustomerName='Jason Young', MemberCategory='C', Age=52, Gender='M', AmountSpent=15.85, Address='blk 48 dorset rd #24-111 singapore 0821', City='Singapore', CountryCode='SIN', ContactTitle='Mr', PhoneNumber=6969696)\n",
            "Row(CustomerID=7345, CustomerName='P Ravichandran', MemberCategory='B', Age=34, Gender='M', AmountSpent=78.73, Address='Blk 612, Clementi West Street 1, #09-290, Singapore 2612', City='Singapore', CountryCode='SIN', ContactTitle='Mr', PhoneNumber=7755113)\n",
            "Row(CustomerID=7856, CustomerName='Rajaram Venkatesh', MemberCategory='A', Age=57, Gender='M', AmountSpent=4.56, Address='Blk 143, Petir road, #07-232, Singapore 3226', City='Singapore', CountryCode='SIN', ContactTitle='Mr', PhoneNumber=7634227)\n",
            "Row(CustomerID=8756, CustomerName='Lu Yun Christine', MemberCategory='B', Age=22, Gender='F', AmountSpent=80.56, Address='Blk 302, Clementi Ave 4, #05-539, Singapore 0302', City='Singapore', CountryCode='SIN', ContactTitle='Ms', PhoneNumber=7759223)\n",
            "Row(CustomerID=9394, CustomerName='Khoo Chee Huat', MemberCategory='B', Age=24, Gender='M', AmountSpent=2.98, Address='Blk 315, Ubi Avenue 1, #05-393, Singapore 1234', City='Singapore', CountryCode='SIN', ContactTitle='Mr', PhoneNumber=8425102)\n",
            "Row(CustomerID=9847, CustomerName='Low Soo Chiew', MemberCategory='B', Age=51, Gender='M', AmountSpent=33.32, Address='Blk 42, Bedok South Road, #05-747, Singapore 1646', City='Singapore', CountryCode='SIN', ContactTitle='Mr', PhoneNumber=4454331)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These examples will give you an array of customers sampled from the original RDD. The actual elements in the sample will vary each time you run the code due to the randomness of the sampling process."
      ],
      "metadata": {
        "id": "nJGdC0dfxpg4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### More Transformations\n",
        "In PySpark, you can perform various RDD operations such as union, join, and cartesian (cross) to combine data in different ways. Here are simple examples for each:\n",
        "#### Union\n",
        "The union operation combines two RDDs to form a new RDD that contains elements from both RDDs."
      ],
      "metadata": {
        "id": "QYMoA6iZx7kL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create two RDDs\n",
        "rdd1 = spark.sparkContext.parallelize([(\"Alice\", 1), (\"Bob\", 2)])\n",
        "rdd2 = spark.sparkContext.parallelize([(\"Charlie\", 3), (\"David\", 4)])\n",
        "\n",
        "# Perform the union operation\n",
        "union_rdd = rdd1.union(rdd2)\n",
        "\n",
        "# Collect and print the results\n",
        "print(union_rdd.collect())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hr-ClfANyEsJ",
        "outputId": "e3815420-473a-442c-9fa1-9e8431447714"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Alice', 1), ('Bob', 2), ('Charlie', 3), ('David', 4)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Join\n",
        "The join operation combines two RDDs based on their key."
      ],
      "metadata": {
        "id": "4V_PebbQyQ2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create two RDDs with common keys\n",
        "rdd3 = spark.sparkContext.parallelize([(\"Alice\", \"Apple\"), (\"Bob\", \"Banana\")])\n",
        "rdd4 = spark.sparkContext.parallelize([(\"Alice\", 1), (\"Bob\", 2)])\n",
        "\n",
        "# Perform the join operation\n",
        "join_rdd = rdd3.join(rdd4)\n",
        "\n",
        "# Collect and print the results\n",
        "print(join_rdd.collect())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsgtMDGHyVK6",
        "outputId": "a77a6041-4f70-4890-9053-4962534dc69b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Alice', ('Apple', 1)), ('Bob', ('Banana', 2))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Cross or Catesian\n",
        "The cartesian operation returns all possible pairs of (a, b) where a is in the first RDD and b is in the second RDD."
      ],
      "metadata": {
        "id": "kwcN4mg6ykED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create two RDDs\n",
        "rdd5 = spark.sparkContext.parallelize([1, 2])\n",
        "rdd6 = spark.sparkContext.parallelize([\"a\", \"b\"])\n",
        "\n",
        "# Perform the cartesian operation\n",
        "cross_rdd = rdd5.cartesian(rdd6)\n",
        "\n",
        "# Collect and print the results\n",
        "print(cross_rdd.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nw643DrQypKe",
        "outputId": "57b43355-5f92-470d-e0e7-e27733fdd438"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(1, 'a'), (1, 'b'), (2, 'a'), (2, 'b')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please note that the cartesian operation can be very expensive in terms of computation and memory usage, especially with large datasets, because it forms all possible combinations of elements between the two RDDs.\n",
        "\n",
        "#### mapValues\n",
        "mapValues transformation is applied to RDD datasets that consist of key-value pairs, and it allows us to transform the value of each pair while keeping the key unchanged. Here's a simple example of how we can use mapValues in PySpark:\n",
        "\n"
      ],
      "metadata": {
        "id": "0cMQA5AkywVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from builtins import sum as builtin_sum\n",
        "# Creating a Pair RDD with student ID as the key and a list of grades as the value\n",
        "studentdata = [(1, [88, 92, 96]), (2, [78, 81, 85]), (3, [68, 72, 74])]\n",
        "studentrdd = spark.sparkContext.parallelize(studentdata)\n",
        "\n",
        "# Function to calculate average\n",
        "def calculate_average(grades):\n",
        "    return builtin_sum(grades) / len(grades)  # Fixed: Use sum() directly\n",
        "\n",
        "# Using mapValues to apply the calculate_average function to each value\n",
        "average_grades_rdd = studentrdd.mapValues(calculate_average)\n",
        "\n",
        "# Print the results directly by collecting the RDD, avoiding .toDF() for now\n",
        "for result in average_grades_rdd.collect():\n",
        "    print(f\"Student ID: {result[0]}, Average Grade: {result[1]:.2f}\")"
      ],
      "metadata": {
        "id": "kclsAEnH4yhb",
        "outputId": "54513642-cb73-4566-9a60-b7837c545d8f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Student ID: 1, Average Grade: 92.00\n",
            "Student ID: 2, Average Grade: 81.33\n",
            "Student ID: 3, Average Grade: 71.33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script will output the average grades for each student ID, maintaining the structure of the RDD with the student ID as the key.\n",
        "\n",
        "#### cogroup\n",
        " The cogroup transformation is used to group data from two or more RDDs based on their key. It returns an RDD consisting of pairs where the key is found in the original RDDs, and the value is a tuple containing Iterable collections of values for that key from each RDD."
      ],
      "metadata": {
        "id": "NZQKqrjUJ89x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create RDD with student ID and names\n",
        "student_names = spark.sparkContext.parallelize([(1, \"John\"), (2, \"Sally\"), (3, \"Bob\")])\n",
        "\n",
        "# Create RDD with student ID and courses\n",
        "courses = spark.sparkContext.parallelize([(1, \"Math\"), (2, \"History\"), (1, \"Biology\"), (3, \"Chemistry\"), (2, \"Physics\")])\n",
        "\n",
        "# CoGroup the RDDs\n",
        "cogrouped_data = student_names.cogroup(courses)\n",
        "\n",
        "# Collect and print results\n",
        "results = cogrouped_data.collect()\n",
        "\n",
        "for student_id, (names, course_list) in results:\n",
        "    print(f\"Student ID: {student_id}\")\n",
        "    print(f\"Name: {list(names)}\")\n",
        "    print(f\"Courses: {list(course_list)}\")\n",
        "    print(\"---\")"
      ],
      "metadata": {
        "id": "nffae1DQKbXO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c832f2b-69f3-4d50-c1e4-0a54c184b95e"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Student ID: 1\n",
            "Name: ['John']\n",
            "Courses: ['Math', 'Biology']\n",
            "---\n",
            "Student ID: 2\n",
            "Name: ['Sally']\n",
            "Courses: ['History', 'Physics']\n",
            "---\n",
            "Student ID: 3\n",
            "Name: ['Bob']\n",
            "Courses: ['Chemistry']\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cogroup operation groups the values for each key in both RDDs into a single pair, where each value is an iterable collection."
      ],
      "metadata": {
        "id": "2lqVk3N6KiN3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### More Actions"
      ],
      "metadata": {
        "id": "VbsOVHatJfv_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####save\n",
        "Saving an RDD in PySpark can be done in a variety of formats. Common formats include saving as text files, sequence files, or other file-based data sources. Below are examples of how to save an RDD that contains customer data as a text file.\n",
        "\n",
        "But we will see about this action after the NoSQL lecture.\n",
        "\n",
        "End of Demo\n",
        "\n",
        "Thank you for the patient listening. "
      ],
      "metadata": {
        "id": "hpntLVNWFmeO"
      }
    }
  ]
}