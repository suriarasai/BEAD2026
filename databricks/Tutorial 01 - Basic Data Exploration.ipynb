{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe4e6aac-86c2-4983-af2d-15ebd3b8ea2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Tutorial 1: Data Exploration & Analysis\n",
    "\n",
    "## Overview\n",
    "This tutorial demonstrates how to load, explore, and understand datasets using Databricks Free Edition. This notebook covers fundamental data exploration techniques including:\n",
    "- Loading and exploring datasets\n",
    "- Data cleaning and quality checks\n",
    "- Statistical analysis and profiling\n",
    "### Learning Objectives\n",
    " - Load data from various sources\n",
    " - Explore data structure and characteristics\n",
    " - Perform basic statistical analysis\n",
    " - Handle common data quality issues\n",
    "\n",
    "**Datasets Used:**\n",
    "- customer_data.csv\n",
    "- products.csv\n",
    "- sales_data.csv\n",
    "- web_traffic.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c95ed0a0-07da-412f-9e31-add5f1be724a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Basic Data Exploration\n",
    "\n",
    "### Loading Data into Databricks\n",
    "\n",
    "**Key Concepts:**\n",
    "- `spark.read.csv()`: Reads CSV files into Spark DataFrame\n",
    "- `header=True`: First row contains column names\n",
    "- `inferSchema=True`: Automatically detects data types\n",
    "- `.toPandas()`: Converts Spark DataFrame to Pandas for easier manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7bbc8f2-0b92-435a-8c36-c25a9c206a84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Load customer data\n",
    "# SYNTAX: spark.read.csv(\"path\", header=True, inferSchema=True)\n",
    "customer_df = spark.read.csv(\"/Volumes/workspace/sample/datasets/customer_data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Display first few rows\n",
    "# SYNTAX: .display() shows data in interactive table format\n",
    "display(customer_df.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9848d3fe-cf46-496e-b462-e062bf054f0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Understanding Your Data\n",
    "\n",
    "**Key Functions:**\n",
    "- `.printSchema()`: Shows column names and data types\n",
    "- `.count()`: Returns number of rows\n",
    "- `.columns`: Lists all column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9cbc0df9-c10c-4524-9824-81730b842e60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check schema and structure\n",
    "print(\"Customer Data Schema:\")\n",
    "customer_df.printSchema()\n",
    "\n",
    "print(f\"\\nTotal Rows: {customer_df.count()}\")\n",
    "print(f\"Total Columns: {len(customer_df.columns)}\")\n",
    "print(f\"\\nColumn Names: {customer_df.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a2bdfa3-8287-484d-8d7f-ccd0c5b6f87a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load other datasets\n",
    "products_df = spark.read.csv(\"/Volumes/workspace/sample/datasets/products.csv\", header=True, inferSchema=True)\n",
    "sales_df = spark.read.csv(\"/Volumes/workspace/sample/datasets/sales_data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "\n",
    "print(\"All datasets loaded successfully!\")\n",
    "print(f\"Products: {products_df.count()} rows\")\n",
    "print(f\"Sales: {sales_df.count()} rows\")\n",
    "print(f\"Customers: {customer_df.count()} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1bf80ef2-a4a3-45ff-a5bf-a4db8a054652",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Quick Data Profiling\n",
    "\n",
    "**Key Functions:**\n",
    "- `.describe()`: Statistical summary for numeric columns\n",
    "- `.summary()`: Extended statistics including percentiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f78f3c4a-163e-45fd-a10b-2c96cda448a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Statistical summary of customer data\n",
    "display(customer_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8096b4e-8b8e-4f04-a201-928e035bde33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# More detailed summary with percentiles\n",
    "display(customer_df.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15fccaaf-99f2-4b31-bbfa-058ee8590116",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Data Cleaning\n",
    "\n",
    "### Handling Missing Values\n",
    "\n",
    "**Key Concepts:**\n",
    "- Check for NULL values using `.isNull().sum()`\n",
    "- Drop nulls with `.na.drop()`\n",
    "- Fill nulls with `.na.fill()`\n",
    "- Replace values with `.na.replace()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69f9898d-5656-4a10-b64a-139a05311a1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check for missing values in customer data\n",
    "from pyspark.sql.functions import col, count, when, isnan\n",
    "\n",
    "# Count nulls for each column\n",
    "# SYNTAX: F.sum(F.when(condition, 1).otherwise(0))\n",
    "missing_counts = customer_df.select([\n",
    "    F.sum(when(col(c).isNull(), 1).otherwise(0)).alias(c) \n",
    "    for c in customer_df.columns\n",
    "])\n",
    "\n",
    "print(\"Missing Values by Column:\")\n",
    "display(missing_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b794f88-91b5-4b1b-bd44-fd67fe6bf46a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Handle missing values - Example strategies\n",
    "\n",
    "# Strategy 1: Drop rows with any null values\n",
    "customer_clean = customer_df.na.drop()\n",
    "print(f\"Rows after dropping nulls: {customer_clean.count()}\")\n",
    "\n",
    "# Strategy 2: Fill missing values with defaults\n",
    "# SYNTAX: .na.fill({\"column_name\": default_value})\n",
    "customer_filled = customer_df.na.fill({\n",
    "    \"phone\": \"Unknown\",\n",
    "    \"email_subscribed\": False,\n",
    "    \"annual_income\": 0\n",
    "})\n",
    "\n",
    "# Strategy 3: Fill with column statistics (mean, median)\n",
    "avg_age = customer_df.select(F.avg(\"age\")).first()[0]\n",
    "customer_filled = customer_df.na.fill({\"age\": int(avg_age)})\n",
    "\n",
    "print(\"Data cleaning strategies applied!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "939e412a-f671-4966-9023-ed0b9d553852",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Handling Duplicates\n",
    "\n",
    "**Key Functions:**\n",
    "- `.dropDuplicates()`: Removes duplicate rows\n",
    "- `.dropDuplicates([columns])`: Removes duplicates based on specific columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67e16bfb-e913-4c89-99db-35b86f183041",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check for duplicate customer records\n",
    "print(f\"Total rows: {customer_df.count()}\")\n",
    "print(f\"Unique customer_ids: {customer_df.select('customer_id').distinct().count()}\")\n",
    "\n",
    "# Remove duplicates based on customer_id\n",
    "customer_unique = customer_df.dropDuplicates(['customer_id'])\n",
    "print(f\"Rows after removing duplicates: {customer_unique.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1388be36-2e3b-4f33-b36f-a2b383b1301f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Data Quality Checks\n",
    "\n",
    "**Best Practices:**\n",
    "- Check for invalid values (negative prices, ages, etc.)\n",
    "- Validate email formats\n",
    "- Check date ranges\n",
    "- Identify outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12b35058-cf08-4a36-867c-23de60c4fdcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Quality checks for customer data\n",
    "\n",
    "# 1. Check for invalid ages\n",
    "invalid_age = customer_df.filter((col(\"age\") < 0) | (col(\"age\") > 120))\n",
    "print(f\"Records with invalid age: {invalid_age.count()}\")\n",
    "\n",
    "# 2. Check annual income range\n",
    "income_stats = customer_df.select(\n",
    "    F.min(\"annual_income\").alias(\"min_income\"),\n",
    "    F.max(\"annual_income\").alias(\"max_income\"),\n",
    "    F.avg(\"annual_income\").alias(\"avg_income\")\n",
    ")\n",
    "display(income_stats)\n",
    "\n",
    "# 3. Check for valid email domains\n",
    "email_pattern = \"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}$\"\n",
    "valid_emails = customer_df.filter(col(\"email\").rlike(email_pattern))\n",
    "print(f\"Valid email addresses: {valid_emails.count()} out of {customer_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e04454b3-aa9b-4db1-9fc8-8a745e723cf3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Handling Outliers\n",
    "\n",
    "**Methods:**\n",
    "- IQR (Interquartile Range) method\n",
    "- Z-score method\n",
    "- Visual inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26ea55d8-4fb1-4a95-94f3-1e5b1c7b3310",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Detect outliers in annual income using IQR method\n",
    "\n",
    "# Calculate quartiles\n",
    "quantiles = customer_df.approxQuantile(\"annual_income\", [0.25, 0.75], 0.01)\n",
    "Q1, Q3 = quantiles[0], quantiles[1]\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define outlier boundaries\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "print(f\"Q1: {Q1:,.0f}, Q3: {Q3:,.0f}, IQR: {IQR:,.0f}\")\n",
    "print(f\"Outlier bounds: [{lower_bound:,.0f}, {upper_bound:,.0f}]\")\n",
    "\n",
    "# Filter outliers\n",
    "outliers = customer_df.filter(\n",
    "    (col(\"annual_income\") < lower_bound) | (col(\"annual_income\") > upper_bound)\n",
    ")\n",
    "print(f\"\\nOutliers detected: {outliers.count()}\")\n",
    "\n",
    "# Remove outliers\n",
    "customer_no_outliers = customer_df.filter(\n",
    "    (col(\"annual_income\") >= lower_bound) & (col(\"annual_income\") <= upper_bound)\n",
    ")\n",
    "print(f\"Records after removing outliers: {customer_no_outliers.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9342446d-fbd9-446d-878e-7e2c16674251",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Statistical Analysis\n",
    "\n",
    "### Descriptive Statistics\n",
    "\n",
    "**Key Metrics:**\n",
    "- Central tendency: mean, median, mode\n",
    "- Dispersion: variance, standard deviation, range\n",
    "- Distribution: skewness, kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2fd84f0c-0e66-4bdd-8b9b-1ebf419849e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Comprehensive statistical analysis\n",
    "\n",
    "# Convert to Pandas for advanced statistics\n",
    "customer_pd = customer_df.select(\"age\", \"annual_income\").toPandas()\n",
    "\n",
    "print(\"=== AGE STATISTICS ===\")\n",
    "print(f\"Mean: {customer_pd['age'].mean():.2f}\")\n",
    "print(f\"Median: {customer_pd['age'].median():.2f}\")\n",
    "print(f\"Mode: {customer_pd['age'].mode()[0]:.2f}\")\n",
    "print(f\"Std Dev: {customer_pd['age'].std():.2f}\")\n",
    "print(f\"Variance: {customer_pd['age'].var():.2f}\")\n",
    "print(f\"Skewness: {customer_pd['age'].skew():.2f}\")\n",
    "\n",
    "print(\"\\n=== INCOME STATISTICS ===\")\n",
    "print(f\"Mean: ${customer_pd['annual_income'].mean():,.2f}\")\n",
    "print(f\"Median: ${customer_pd['annual_income'].median():,.2f}\")\n",
    "print(f\"Std Dev: ${customer_pd['annual_income'].std():,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8321be93-70b9-4557-92e9-4089b2abbe6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Group-wise Analysis\n",
    "\n",
    "**Key Functions:**\n",
    "- `.groupBy()`: Group data by one or more columns\n",
    "- `.agg()`: Apply aggregate functions\n",
    "- Common aggregations: count, sum, avg, min, max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "204d1c5c-5482-4f14-aac3-8a6ee9fe3b6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Analyze customers by segment\n",
    "segment_analysis = customer_df.groupBy(\"segment\").agg(\n",
    "    F.count(\"customer_id\").alias(\"customer_count\"),\n",
    "    F.avg(\"age\").alias(\"avg_age\"),\n",
    "    F.avg(\"annual_income\").alias(\"avg_income\"),\n",
    "    F.sum(when(col(\"email_subscribed\") == True, 1).otherwise(0)).alias(\"subscribed_count\")\n",
    ").orderBy(F.desc(\"customer_count\"))\n",
    "\n",
    "display(segment_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9839c28-8fc6-47d8-85d2-caa7e4ba06f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Geographic analysis by state\n",
    "state_analysis = customer_df.groupBy(\"state\").agg(\n",
    "    F.count(\"customer_id\").alias(\"customer_count\"),\n",
    "    F.avg(\"annual_income\").alias(\"avg_income\")\n",
    ").orderBy(F.desc(\"customer_count\")).limit(10)\n",
    "\n",
    "display(state_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eaeeeca0-81b0-4d8e-bedc-139fa960284d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Correlation Analysis\n",
    "\n",
    "**Purpose:** Understand relationships between numeric variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "928a6dbd-ff3d-4dad-a597-5d982bf6c0cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate correlation between age and income\n",
    "correlation = customer_df.stat.corr(\"age\", \"annual_income\")\n",
    "print(f\"Correlation between Age and Income: {correlation:.3f}\")\n",
    "\n",
    "# Create correlation matrix using Pandas\n",
    "numeric_cols = [\"age\", \"annual_income\"]\n",
    "correlation_matrix = customer_df.select(numeric_cols).toPandas().corr()\n",
    "print(\"\\nCorrelation Matrix:\")\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2197f75-bc40-4de3-b377-6798f029cb0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Product Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19a3fb97-b270-4f49-a01b-348f153e01f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Analyze product data\n",
    "product_summary = products_df.agg(\n",
    "    F.count(\"product_id\").alias(\"total_products\"),\n",
    "    F.avg(\"price\").alias(\"avg_price\"),\n",
    "    F.avg(\"rating\").alias(\"avg_rating\"),\n",
    "    F.sum(\"num_reviews\").alias(\"total_reviews\")\n",
    ")\n",
    "\n",
    "display(product_summary)\n",
    "\n",
    "# Products by category\n",
    "category_analysis = products_df.groupBy(\"category\").agg(\n",
    "    F.count(\"product_id\").alias(\"product_count\"),\n",
    "    F.avg(\"price\").alias(\"avg_price\"),\n",
    "    F.avg(\"rating\").alias(\"avg_rating\")\n",
    ").orderBy(F.desc(\"product_count\"))\n",
    "\n",
    "display(category_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fbe2c467-7325-4f2e-a7a0-4df7142944c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Sales Data Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ca50b1e-8ec0-45d2-b3c8-2e5a0acf48f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sales performance metrics\n",
    "sales_summary = sales_df.agg(\n",
    "    F.count(\"transaction_id\").alias(\"total_transactions\"),\n",
    "    F.sum(\"total_sales\").alias(\"total_revenue\"),\n",
    "    F.avg(\"total_sales\").alias(\"avg_transaction_value\"),\n",
    "    F.avg(\"customer_satisfaction\").alias(\"avg_satisfaction\"),\n",
    "    F.sum(\"quantity\").alias(\"total_units_sold\")\n",
    ")\n",
    "\n",
    "display(sales_summary)\n",
    "\n",
    "# Regional performance\n",
    "regional_analysis = sales_df.groupBy(\"region\").agg(\n",
    "    F.count(\"transaction_id\").alias(\"transactions\"),\n",
    "    F.sum(\"total_sales\").alias(\"revenue\"),\n",
    "    F.avg(\"customer_satisfaction\").alias(\"avg_satisfaction\")\n",
    ").orderBy(F.desc(\"revenue\"))\n",
    "\n",
    "display(regional_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "969e3430-c1a0-4403-bc97-51894c7f10ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Key Takeaways\n",
    "\n",
    "**Data Exploration:**\n",
    "- Always start with `.printSchema()` and `.describe()` to understand your data\n",
    "- Check data quality early: missing values, duplicates, outliers\n",
    "\n",
    "**Data Cleaning:**\n",
    "- Handle missing values appropriately (drop, fill, or impute)\n",
    "- Validate data ranges and formats\n",
    "- Remove or treat outliers based on business context\n",
    "\n",
    "**Statistical Analysis:**\n",
    "- Use descriptive statistics to understand distributions\n",
    "- Group-wise analysis reveals patterns across categories\n",
    "- Correlation helps identify relationships between variables\n",
    "\n",
    "**Next Steps:** Move to Notebook 2 for Data Visualization techniques!"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Tutorial 01 - Basic Data Exploration",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}